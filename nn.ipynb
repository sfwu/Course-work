{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here!\n",
      "100\n",
      "torch.Size([1878, 100])\n",
      "torch.Size([476, 100])\n"
     ]
    }
   ],
   "source": [
    "print(\"I am here!\")\n",
    "\n",
    "my_data = pd.read_csv(\"train.csv\", index_col = 0)\n",
    "\n",
    "my_data = my_data.drop(columns = ['batch_size_test', 'batch_size_val', 'criterion', 'optimizer', 'batch_size_train'])\n",
    "y_train = my_data['train_error']\n",
    "y_val = my_data['val_error']\n",
    "my_data = my_data.drop(columns = 'train_error')\n",
    "my_data = my_data.drop(columns = 'val_error')\n",
    "\n",
    "x = my_data.drop(columns = ['train_loss','val_loss','arch_and_hp','init_params_mu', 'init_params_std', 'init_params_l2'])\n",
    "\n",
    "col_train = [col for col in x.filter(regex='^train',axis=1).columns]\n",
    "col_val = [col for col in x.filter(regex='^val',axis=1).columns]\n",
    "#print(col_train)\n",
    "#print(col_val)\n",
    "#print(x)\n",
    "\n",
    "torch_tensor_train = torch.tensor([x[title].values for title in col_train],dtype=torch.float)\n",
    "torch_tensor_val = torch.tensor([x[title].values for title in col_val],dtype=torch.float)\n",
    "\n",
    "\n",
    "torch_tensor_train = torch.t(torch_tensor_train)\n",
    "torch_tensor_val = torch.t(torch_tensor_val)\n",
    "\n",
    "\n",
    "print(len(col_train))\n",
    "print(torch_tensor_train.size())\n",
    "\n",
    "# ------------- test part -----------------------\n",
    "\n",
    "test_data = pd.read_csv(\"test.csv\", index_col = 0)\n",
    "test_data = test_data.drop(columns = ['batch_size_test', 'batch_size_val', 'criterion', 'optimizer', 'batch_size_train'])\n",
    "test_data = test_data.drop(columns = ['arch_and_hp','init_params_mu', 'init_params_std', 'init_params_l2'])\n",
    "test_train = test_data[col_train]\n",
    "test_val = test_data[col_val]\n",
    "\n",
    "torch_tensor_test_train = torch.tensor([test_train[title].values for title in col_train],dtype=torch.float)\n",
    "torch_tensor_test_val = torch.tensor([test_val[title].values for title in col_val],dtype=torch.float)\n",
    "\n",
    "torch_tensor_test_train = torch.t(torch_tensor_test_train)\n",
    "torch_tensor_test_val = torch.t(torch_tensor_test_val)\n",
    "\n",
    "#print(test_train)\n",
    "print(torch_tensor_test_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Wusongfeng/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "#we first select a naive model after sorting based on the lowest val_error\n",
    "\n",
    "#model = torch.nn.Sequential(nn.BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), nn.Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1)) , nn.Tanh() , nn.Dropout2d(p=0.4569261378573387, inplace=False) , nn.Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1)) , nn.ReLU() , nn.Dropout2d(p=0.022897457910152297, inplace=False) , nn.Conv2d(27, 28, kernel_size=(5, 5), stride=(1, 1)) , nn.LeakyReLU(negative_slope=0.5719652455913541) , nn.MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False) , nn.Flatten() , nn.Linear(in_features=700, out_features=28, bias=True) , nn.LeakyReLU(negative_slope=0.48187737007312437) , nn.Dropout(p=0.3735548245474202, inplace=False) , nn.Linear(in_features=28, out_features=10, bias=True) , nn.BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) )\n",
    "\n",
    "\n",
    "input_size = 100\n",
    "hidden_sizes = [100, 100]\n",
    "output_size = 1\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n",
    "\n",
    "\n",
    "model_train = nn.Sequential(\n",
    "                      nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      )\n",
    "#nn.Softmax(dim=0)\n",
    "# Define the loss\n",
    "criterion = nn.MSELoss()\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = torch.optim.SGD(model_train.parameters(), lr=0.03)\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(1878):\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model_train(torch_tensor_train[i])\n",
    "        #print(output)\n",
    "        #torch.tensor(y_train[i])\n",
    "        loss = criterion(output, torch.tensor(y_train[i]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        \n",
    "model_val = nn.Sequential(\n",
    "                      nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      )\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = torch.optim.SGD(model_val.parameters(), lr=0.03)\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(1878):\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model_val(torch_tensor_val[i])\n",
    "        #print(output)\n",
    "        loss = criterion(output, torch.tensor(y_val[i]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "print(\"finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here!\n",
      "[0.5583013296127319, 0.4050866365432739, 0.5262130498886108, 0.680777907371521, 0.5923596024513245, 0.5499338507652283, 0.7253270745277405, 0.42069774866104126, 0.6739445328712463, 0.31737637519836426, 0.46779578924179077, 0.3898645043373108, 0.36771681904792786, 0.49539193511009216, 0.4169318974018097, 0.5172078609466553, 0.4393386244773865, 0.43747478723526, 0.38684356212615967, 0.4562339782714844, 0.3852398097515106, 0.4175161123275757, 0.6088973879814148, 0.28341975808143616, 0.5611382126808167, 0.47171351313591003, 0.6685824394226074, 0.3603111207485199, 0.45174649357795715, 0.3378792703151703, 0.7338246703147888, 0.38442370295524597, 0.6988584399223328, 0.4749460518360138, 0.4936230480670929, 0.6209245324134827, 0.5204269886016846, 0.625515878200531, 0.46746188402175903, 0.45091986656188965, 0.6439609527587891, 0.6431218981742859, 0.7168214917182922, 0.6156345009803772, 0.45250609517097473, 0.38995665311813354, 0.5342491269111633, 0.44317930936813354, 0.5139147639274597, 0.4938519597053528, 0.32830724120140076, 0.6588965058326721, 0.7185112237930298, 0.543370246887207, 0.5397221446037292, 0.45221173763275146, 0.4476028382778168, 0.3923879861831665, 0.6822511553764343, 0.45597341656684875, 0.48068177700042725, 0.3648962080478668, 0.4356725215911865, 0.3770657181739807, 0.5286626219749451, 0.3747463822364807, 0.5804442167282104, 0.37750932574272156, 0.443011075258255, 0.377044141292572, 0.3140065371990204, 0.4622739553451538, 0.44661861658096313, 0.48423266410827637, 0.5247300863265991, 0.5235803127288818, 0.6640122532844543, 0.4027468264102936, 0.39513370394706726, 0.39781349897384644, 0.4398226737976074, 0.5509318113327026, 0.6277276873588562, 0.7183700203895569, 0.5613550543785095, 0.4061875343322754, 0.4549112915992737, 0.40116044878959656, 0.42203229665756226, 0.45814430713653564, 0.42521747946739197, 0.339765340089798, 0.6752821803092957, 0.4018457531929016, 0.4830625653266907, 0.4000667929649353, 0.44782742857933044, 0.4456750750541687, 0.666637659072876, 0.3004775643348694, 0.34508374333381653, 0.4227057993412018, 0.46919456124305725, 0.5050750970840454, 0.5700538158416748, 0.7169164419174194, 0.37440574169158936, 0.6773308515548706, 0.6718459725379944, 0.42774176597595215, 0.5328091979026794, 0.46758490800857544, 0.5027990341186523, 0.47915220260620117, 0.720054030418396, 0.45675748586654663, 0.4331521987915039, 0.6698471903800964, 0.43765372037887573, 0.46551454067230225, 0.4381697177886963, 0.7162512540817261, 0.35813817381858826, 0.6456896066665649, 0.45870858430862427, 0.5555452704429626, 0.34742265939712524, 0.4099813997745514, 0.5756421685218811, 0.48784714937210083, 0.5387342572212219, 0.44790124893188477, 0.47596925497055054, 0.44271886348724365, 0.5573732852935791, 0.6262903213500977, 0.41534197330474854, 0.7191367149353027, 0.45362141728401184, 0.6365557909011841, 0.5993895530700684, 0.6681713461875916, 0.47086095809936523, 0.4264185428619385, 0.3594479560852051, 0.4759453535079956, 0.4988420605659485, 0.3637753129005432, 0.7174526453018188, 0.620772659778595, 0.5210592746734619, 0.6035771369934082, 0.5920534133911133, 0.545683741569519, 0.5648977756500244, 0.4618246853351593, 0.4596406817436218, 0.7195978760719299, 0.7102285623550415, 0.3739815354347229, 0.5279934406280518, 0.40883541107177734, 0.38933494687080383, 0.33954355120658875, 0.4066803455352783, 0.476327121257782, 0.542570948600769, 0.4334966540336609, 0.35661211609840393, 0.4308723509311676, 0.7333456873893738, 0.40948426723480225, 0.44352322816848755, 0.5126689672470093, 0.46191519498825073, 0.5213429927825928, 0.5445860624313354, 0.5152745246887207, 0.6612953543663025, 0.3494158685207367, 0.6774618625640869, 0.6212054491043091, 0.6693771481513977, 0.7160801887512207, 0.33438828587532043, 0.717339277267456, 0.6346927285194397, 0.4166340231895447, 0.4250853955745697, 0.3934100866317749, 0.7063231468200684, 0.3182916045188904, 0.4643445611000061, 0.6822397112846375, 0.5222645401954651, 0.4561060667037964, 0.3924870193004608, 0.5304248332977295, 0.4852842390537262, 0.7057425379753113, 0.4714016616344452, 0.6229050159454346, 0.28148263692855835, 0.6672036051750183, 0.5507826805114746, 0.4097314476966858, 0.4786040186882019, 0.6137060523033142, 0.6489446759223938, 0.451479971408844, 0.5590848326683044, 0.4342797100543976, 0.45851999521255493, 0.5363736748695374, 0.636566162109375, 0.5244415998458862, 0.4463390111923218, 0.4196397066116333, 0.7193003296852112, 0.41775405406951904, 0.7194798588752747, 0.5020455121994019, 0.461948424577713, 0.36175188422203064, 0.4879785180091858, 0.5220655202865601, 0.5093404650688171, 0.5048458576202393, 0.46527421474456787, 0.47454315423965454, 0.5277222394943237, 0.4405893087387085, 0.6193006038665771, 0.5283930897712708, 0.7109018564224243, 0.28469118475914, 0.5100852251052856, 0.4468044638633728, 0.3706035912036896, 0.7192105650901794, 0.43384093046188354, 0.4623188376426697, 0.49585774540901184, 0.4902763366699219, 0.58908611536026, 0.5218324065208435, 0.4702802896499634, 0.3598731756210327, 0.7177165746688843, 0.36554208397865295, 0.35333341360092163, 0.5283368825912476, 0.49969005584716797, 0.3488326668739319, 0.42581674456596375, 0.4538661241531372, 0.3167469799518585, 0.5475192666053772, 0.3757798969745636, 0.45077893137931824, 0.5808447599411011, 0.4536619186401367, 0.38694730401039124, 0.4689183235168457, 0.591458261013031, 0.4615781009197235, 0.4150049686431885, 0.45359092950820923, 0.6140074729919434, 0.4276024103164673, 0.3608415424823761, 0.4347502589225769, 0.38831010460853577, 0.6127884387969971, 0.7132314443588257, 0.3420052230358124, 0.5163230895996094, 0.44161170721054077, 0.7167274951934814, 0.3588780164718628, 0.497725248336792, 0.4791560173034668, 0.5987951159477234, 0.3188110291957855, 0.433100163936615, 0.42206886410713196, 0.3485524356365204, 0.4672625660896301, 0.6061462163925171, 0.47566646337509155, 0.4462200999259949, 0.41656455397605896, 0.4115525484085083, 0.5559588074684143, 0.474079966545105, 0.5174139142036438, 0.2890779674053192, 0.4315975308418274, 0.5504842400550842, 0.43321335315704346, 0.4107145667076111, 0.371150404214859, 0.4592783749103546, 0.37031495571136475, 0.47402286529541016, 0.3867069482803345, 0.7216016054153442, 0.5710511207580566, 0.4121960997581482, 0.47414854168891907, 0.5069616436958313, 0.6626647710800171, 0.40415140986442566, 0.5961678624153137, 0.6480870842933655, 0.46164634823799133, 0.4614375829696655, 0.43022316694259644, 0.6267511248588562, 0.6038422584533691, 0.4286942481994629, 0.6971787810325623, 0.4489441514015198, 0.4998268485069275, 0.48926931619644165, 0.7165284752845764, 0.6985014081001282, 0.7195020914077759, 0.42515993118286133, 0.41914933919906616, 0.5334200859069824, 0.6413270831108093, 0.6034966707229614, 0.5040074586868286, 0.4918819069862366, 0.44356417655944824, 0.35507524013519287, 0.598171055316925, 0.4269678592681885, 0.4436114430427551, 0.6769226789474487, 0.41453221440315247, 0.49266743659973145, 0.6682789921760559, 0.5676822066307068, 0.46550554037094116, 0.4031692147254944, 0.5412154197692871, 0.361892431974411, 0.47113990783691406, 0.6673310399055481, 0.48352479934692383, 0.3628024160861969, 0.38747966289520264, 0.6527957320213318, 0.47991466522216797, 0.7195268273353577, 0.6105002164840698, 0.36210018396377563, 0.7152184247970581, 0.41777467727661133, 0.3708459138870239, 0.45001620054244995, 0.5016175508499146, 0.2883587181568146, 0.43844038248062134, 0.4394106864929199, 0.5178804397583008, 0.43251368403434753, 0.6859149932861328, 0.4665408730506897, 0.71770179271698, 0.398602694272995, 0.38534924387931824, 0.5765795707702637, 0.6047075986862183, 0.4672842025756836, 0.5475033521652222, 0.3785771131515503, 0.49800118803977966, 0.4953697919845581, 0.5076562166213989, 0.4846508502960205, 0.48739224672317505, 0.41570693254470825, 0.4519999027252197, 0.3937290608882904, 0.7256482839584351, 0.7260673642158508, 0.5297375917434692, 0.5053389668464661, 0.47869980335235596, 0.7180787920951843, 0.5050007700920105, 0.388434499502182, 0.2972606122493744, 0.4242701530456543, 0.47927358746528625, 0.6429222822189331, 0.4748292565345764, 0.45399010181427, 0.6983969807624817, 0.6123247146606445, 0.5720614194869995, 0.47338441014289856, 0.5289159417152405, 0.4574761390686035, 0.7057844996452332, 0.41968023777008057, 0.6105832457542419, 0.4591931402683258, 0.4375150799751282, 0.518284797668457, 0.7188679575920105, 0.47451525926589966, 0.3551348149776459, 0.42660656571388245, 0.44561484456062317, 0.3637940585613251, 0.6595259308815002, 0.6562018990516663, 0.5082682967185974, 0.4290774166584015, 0.4155433773994446, 0.5270799398422241, 0.5109586715698242, 0.4484720826148987, 0.5008302927017212, 0.5232717394828796, 0.497841477394104, 0.34179383516311646, 0.4472755491733551, 0.4475746750831604, 0.3991563022136688, 0.4765929579734802, 0.7208482027053833, 0.5333723425865173, 0.4261404275894165, 0.3671290874481201, 0.4386741518974304, 0.459608793258667, 0.7196326851844788, 0.43295538425445557, 0.4713948369026184, 0.4283055365085602, 0.43928468227386475, 0.5191067457199097, 0.38882043957710266, 0.4688159227371216, 0.4435388743877411, 0.43750232458114624, 0.7164472937583923, 0.3744999170303345, 0.44669753313064575, 0.42597025632858276, 0.6426881551742554, 0.44432276487350464, 0.42332738637924194, 0.5381635427474976, 0.34603339433670044, 0.5462310314178467, 0.41990548372268677, 0.35337454080581665, 0.4448127746582031, 0.5207051634788513, 0.4894794225692749, 0.4539341330528259, 0.44855228066444397, 0.39174288511276245, 0.6992207765579224, 0.46572452783584595, 0.3766569197177887, 0.5296921730041504, 0.39611074328422546, 0.4532269239425659, 0.4712018370628357]\n",
      "[0.8674935102462769, 0.37448039650917053, 0.606041669845581, 0.839719831943512, 0.8363857269287109, 0.5161624550819397, 0.892339289188385, 0.3757632076740265, 0.8018752932548523, 0.17791856825351715, 0.3993239402770996, 0.3952437937259674, -0.013972312211990356, 0.600002110004425, 0.23467394709587097, 0.4421367943286896, 0.26883095502853394, 0.4371820390224457, 0.18087570369243622, 0.08252394199371338, 0.0062631964683532715, 0.5075421333312988, 0.7679272294044495, -0.004071056842803955, 0.6364090442657471, 0.5238831639289856, 0.8154798746109009, -0.0032761096954345703, 0.04060712456703186, 0.2904130220413208, 0.8697911500930786, 0.21446827054023743, 0.7919706106185913, 0.06310465931892395, 0.021704882383346558, 0.7203773260116577, 0.544821560382843, 0.8017235994338989, 0.5874997973442078, 0.1702907532453537, 0.6024312973022461, 0.8090753555297852, 0.8663702607154846, 0.862802267074585, 0.14987057447433472, 0.03373005986213684, 0.2017425149679184, 0.23295778036117554, 0.4534463882446289, 0.5663878321647644, 0.6949076652526855, 0.8769327998161316, 0.8884215950965881, 0.6912020444869995, 0.5837256908416748, 0.028568565845489502, 0.5262755155563354, 0.0025679171085357666, 0.8632525205612183, 0.006917178630828857, 0.42663031816482544, 0.005646109580993652, 0.40815356373786926, 0.056738048791885376, 0.6598266959190369, 0.012314438819885254, 0.691875159740448, 0.31409627199172974, 0.19342701137065887, 0.54173344373703, 0.28847068548202515, 0.1982010006904602, 0.33251622319221497, 0.5938418507575989, 0.6604723334312439, 0.6750110387802124, 0.8368750810623169, 0.5862020254135132, 0.3207416534423828, 0.3586743474006653, 0.19593261182308197, 0.6872802376747131, 0.7111401557922363, 0.8671878576278687, 0.10939818620681763, 0.4739183783531189, -0.010530203580856323, 0.0035209357738494873, 0.1818888634443283, 0.692497968673706, 0.2076997458934784, 0.3445563018321991, 0.8362423777580261, 0.3222329914569855, 0.31110143661499023, 0.813344419002533, 0.20810455083847046, 0.5110177993774414, 0.8064956068992615, 0.08840340375900269, -0.0037072300910949707, 0.3424881398677826, 0.19900338351726532, 0.5925157070159912, 0.23109237849712372, 0.8681433796882629, 0.04269540309906006, 0.5099079608917236, 0.8576620221138, 0.4528220295906067, 0.8641667366027832, 0.046139806509017944, 0.5188887715339661, 0.5491183996200562, 0.8664305806159973, 0.06560781598091125, 0.5412414073944092, 0.7848339080810547, 0.450199693441391, 0.06006845831871033, 0.7241685390472412, 0.8679977655410767, -0.016508370637893677, 0.7983849048614502, 0.4027174711227417, 0.6538271903991699, 0.8271164894104004, 0.3682423233985901, 0.7391877174377441, 0.3530450165271759, 0.7005065679550171, 0.30157503485679626, 0.25898200273513794, 0.2911338210105896, 0.6210163831710815, 0.7344915866851807, 0.46468889713287354, 0.8669135570526123, 0.2013615518808365, 0.8387508988380432, 0.8407895565032959, 0.8732027411460876, 0.013801097869873047, 0.13305030763149261, 0.19413205981254578, 0.5431247353553772, 0.10186150670051575, 0.03364145755767822, 0.8184230327606201, 0.7405890822410583, 0.6203446388244629, 0.8438152074813843, 0.7742919325828552, 0.5786360502243042, 0.5470423102378845, 0.16733676195144653, 0.1482955813407898, 0.8681687712669373, 0.8490023016929626, 0.10395586490631104, 0.5018420219421387, 0.053651005029678345, 0.846721887588501, -0.006488144397735596, 0.3640756607055664, 0.14770060777664185, 0.861762285232544, 0.03645309805870056, -0.008247673511505127, 0.45716843008995056, 0.9245265126228333, 0.27670806646347046, 0.45161017775535583, 0.5616537928581238, 0.46112769842147827, 0.7459181547164917, 0.6829972267150879, 0.7065185308456421, 0.8462326526641846, 0.10909497737884521, 0.779507040977478, 0.707788348197937, 0.8858520984649658, 0.8735885620117188, 0.47825589776039124, 0.8694868087768555, 0.5099153518676758, 0.6433054804801941, 0.33225807547569275, 0.3714010417461395, 0.8628333806991577, 0.34293198585510254, 0.08357521891593933, 0.7930579781532288, 0.363162636756897, 0.5439240336418152, 0.015665769577026367, 0.4579276740550995, 0.5347132682800293, 0.8750395774841309, -0.0028300583362579346, 0.8969489932060242, 0.18745534121990204, 0.8654069900512695, 0.7997112274169922, 0.5856009125709534, 0.2801051139831543, 0.8934815526008606, 0.5789399147033691, 0.30439409613609314, 0.7854295969009399, 0.4238857626914978, 0.5000733137130737, 0.5021061301231384, 0.8399476408958435, 0.8497105836868286, 0.673187255859375, 0.3643718361854553, 0.8789616823196411, 0.2121157944202423, 0.8683038949966431, 0.7107232809066772, 0.3009899854660034, -0.01898154616355896, 0.20424441993236542, 0.6450804471969604, 0.6745140552520752, 0.6812421083450317, 0.3842269778251648, 0.48591354489326477, 0.6423985958099365, 0.3115672469139099, 0.813642144203186, 0.6642622947692871, 0.8687472343444824, 0.1772002875804901, 0.047523438930511475, 0.6079540848731995, 0.37403902411460876, 0.8767417073249817, 0.42309826612472534, 0.05689290165901184, 0.4802934527397156, 0.5565150380134583, 0.6176610589027405, 0.7589097023010254, 0.5470346212387085, 0.05483657121658325, 0.8671218156814575, 0.1532469391822815, 0.26527369022369385, 0.7319413423538208, 0.7490963935852051, 0.800229012966156, 0.3908200263977051, 0.02435871958732605, 0.14598464965820312, 0.6989970207214355, 0.21062837541103363, -0.003167390823364258, 0.8066903352737427, 0.07476860284805298, 0.4801505208015442, 0.028758108615875244, 0.37405261397361755, 0.5389415621757507, 0.20489086210727692, 0.16022159159183502, 0.8141621947288513, 0.4710988998413086, 0.2958603501319885, 0.10571756958961487, 0.28097644448280334, 0.8026511669158936, 0.6599488258361816, 0.06051012873649597, 0.622170090675354, 0.24838897585868835, 0.8670112490653992, 0.06076720356941223, -0.0014690756797790527, 0.19473163783550262, 0.5159647464752197, 0.2444845736026764, 0.4033702313899994, 0.4159483313560486, 0.5596891045570374, 0.1049400269985199, 0.8387250900268555, 0.1927453726530075, 0.3458769619464874, 0.08566609025001526, 0.5994242429733276, 0.6788815259933472, 0.7386752367019653, 0.5966496467590332, 0.032331615686416626, 0.02870890498161316, 0.6401040554046631, 0.2872347831726074, -0.0035967230796813965, 0.13494689762592316, 0.5026884078979492, 0.18612593412399292, 0.1463424265384674, 0.4186864495277405, 0.8944950699806213, 0.6250249743461609, 0.33216819167137146, 0.3438829481601715, 0.6148034334182739, 0.8351854085922241, 0.19787803292274475, 0.6996355056762695, 0.8105733394622803, -0.0017491579055786133, 0.747249960899353, 0.35161107778549194, 0.8389863967895508, 0.655507504940033, 0.3057466149330139, 0.5954740047454834, 0.29740598797798157, 0.5547422766685486, 0.04808363318443298, 0.8648165464401245, 0.8732864856719971, 0.8668125867843628, 0.3573838174343109, 0.37441498041152954, 0.6504467725753784, 0.8133556842803955, 0.6900038719177246, 0.41546207666397095, 0.10715019702911377, 0.00764349102973938, 0.018070727586746216, 0.7381489276885986, 0.48807260394096375, 0.2716771960258484, 0.7927982807159424, 0.4196668863296509, 0.23105034232139587, 0.8680886030197144, 0.6444331407546997, 0.3014295995235443, 0.7668697834014893, 0.7164608240127563, 0.3666890561580658, 0.17474700510501862, 0.374851793050766, 0.6347635984420776, 0.2763115167617798, 0.49283915758132935, 0.7053964734077454, 0.500521183013916, 0.8663094639778137, 0.7692297697067261, 0.05438363552093506, 0.8679075837135315, 0.2329980880022049, 0.016505450010299683, 0.670021653175354, 0.8278090953826904, 0.04405057430267334, 0.5001059174537659, 0.6215667724609375, 0.7030923962593079, 0.5248002409934998, 0.7667711973190308, 0.36868375539779663, 0.8663575053215027, 0.029240965843200684, 0.14551347494125366, 0.8139698505401611, 0.8604669570922852, 0.18374210596084595, 0.8033779263496399, 0.36933428049087524, 0.616855800151825, 0.16964896023273468, 0.3736325800418854, 0.5729770660400391, 0.4174157679080963, 0.11856810748577118, 0.6010725498199463, 0.1330508142709732, 0.8645546436309814, 0.8708839416503906, 0.6877962350845337, 0.5958442091941833, 0.48502397537231445, 0.8676000237464905, -0.031278759241104126, -0.009625464677810669, 0.1908968687057495, 0.43704795837402344, 0.21569815278053284, 0.5766212940216064, 0.548782229423523, -0.0008279979228973389, 0.8644710183143616, 0.7706116437911987, 0.7948625087738037, 0.6050262451171875, 0.45834967494010925, 0.6609683036804199, 0.8543841242790222, 0.08365398645401001, 0.7840465307235718, 0.006426572799682617, 0.4557303488254547, 0.4585638642311096, 0.8871651291847229, 0.6706734895706177, -0.026461511850357056, 0.6076605319976807, 0.4389970600605011, 0.0038742125034332275, 0.8062258958816528, 0.8910598754882812, 0.4940183162689209, 0.7828136086463928, 0.25441908836364746, 0.6805628538131714, 0.6589707732200623, 0.5799700021743774, 0.8348987102508545, 0.6330373287200928, 0.5073449611663818, 0.01584509015083313, -0.0015393495559692383, 0.015865027904510498, 0.21973277628421783, 0.8877238035202026, 0.9034309387207031, 0.2384442538022995, 0.3848208487033844, -0.02261805534362793, 0.2957524061203003, 0.1940268576145172, 0.8660261631011963, 0.2684493362903595, 0.5136345624923706, 0.3397468030452728, 0.5133618116378784, 0.004461169242858887, 0.546811580657959, 0.41456207633018494, 0.443737268447876, 0.5300428867340088, 0.8659827709197998, 0.015510231256484985, 0.48155200481414795, 0.35548532009124756, 0.8336992263793945, 0.37163105607032776, 0.4872910976409912, 0.009829998016357422, -0.023398011922836304, 0.6966021060943604, 0.5042619705200195, 0.21558648347854614, 0.3236545920372009, 0.602386474609375, 0.49094200134277344, 0.6135761737823486, 0.3984702527523041, 0.2743997275829315, 0.6554239988327026, 0.31567442417144775, 0.02734816074371338, 0.7136968970298767, 0.02546176314353943, 0.34599754214286804, 0.4249263405799866]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#now we do the prediction\n",
    "\n",
    "print(\"I am here!\")\n",
    "res_train,res_val = [],[]\n",
    "\n",
    "for row in torch_tensor_test_train:\n",
    "    res_train.append(model_train(row).item())\n",
    "    \n",
    "for row in torch_tensor_test_val:\n",
    "    res_val.append(model_val(row).item())\n",
    "\n",
    "print(res_val)\n",
    "print(res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(\"sample_submission.csv\", index_col = 0)\n",
    "res_array = np.zeros(len(res_train)*2)\n",
    "submit.Predicted = res_array\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while True:\n",
    "    \n",
    "    if i == len(res_train):\n",
    "        break\n",
    "    \n",
    "    res_array[j] = res_val[i]\n",
    "    j += 1\n",
    "    res_array[j] = res_train[i]\n",
    "    j += 1\n",
    "    \n",
    "    i += 1 \n",
    "    \n",
    "submit.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
