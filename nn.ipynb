{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here!\n",
      "100\n",
      "torch.Size([1878, 100])\n",
      "torch.Size([476, 100])\n"
     ]
    }
   ],
   "source": [
    "print(\"I am here!\")\n",
    "\n",
    "my_data = pd.read_csv(\"train.csv\", index_col = 0)\n",
    "\n",
    "my_data = my_data.drop(columns = ['batch_size_test', 'batch_size_val', 'criterion', 'optimizer', 'batch_size_train'])\n",
    "y_train = my_data['train_error']\n",
    "y_val = my_data['val_error']\n",
    "my_data = my_data.drop(columns = 'train_error')\n",
    "my_data = my_data.drop(columns = 'val_error')\n",
    "\n",
    "x = my_data.drop(columns = ['train_loss','val_loss','arch_and_hp','init_params_mu', 'init_params_std', 'init_params_l2'])\n",
    "\n",
    "col_train = [col for col in x.filter(regex='^train',axis=1).columns]\n",
    "col_val = [col for col in x.filter(regex='^val',axis=1).columns]\n",
    "#print(col_train)\n",
    "#print(col_val)\n",
    "#print(x)\n",
    "\n",
    "torch_tensor_train = torch.tensor([x[title].values for title in col_train],dtype=torch.float)\n",
    "torch_tensor_val = torch.tensor([x[title].values for title in col_val],dtype=torch.float)\n",
    "\n",
    "\n",
    "torch_tensor_train = torch.t(torch_tensor_train)\n",
    "torch_tensor_val = torch.t(torch_tensor_val)\n",
    "\n",
    "\n",
    "print(len(col_train))\n",
    "print(torch_tensor_train.size())\n",
    "\n",
    "# ------------- test part -----------------------\n",
    "\n",
    "test_data = pd.read_csv(\"test.csv\", index_col = 0)\n",
    "test_data = test_data.drop(columns = ['batch_size_test', 'batch_size_val', 'criterion', 'optimizer', 'batch_size_train'])\n",
    "test_data = test_data.drop(columns = ['arch_and_hp','init_params_mu', 'init_params_std', 'init_params_l2'])\n",
    "test_train = test_data[col_train]\n",
    "test_val = test_data[col_val]\n",
    "\n",
    "torch_tensor_test_train = torch.tensor([test_train[title].values for title in col_train],dtype=torch.float)\n",
    "torch_tensor_test_val = torch.tensor([test_val[title].values for title in col_val],dtype=torch.float)\n",
    "\n",
    "torch_tensor_test_train = torch.t(torch_tensor_test_train)\n",
    "torch_tensor_test_val = torch.t(torch_tensor_test_val)\n",
    "\n",
    "#print(test_train)\n",
    "print(torch_tensor_test_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Wusongfeng/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "#we first select a naive model after sorting based on the lowest val_error\n",
    "\n",
    "#model = torch.nn.Sequential(nn.BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), nn.Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1)) , nn.Tanh() , nn.Dropout2d(p=0.4569261378573387, inplace=False) , nn.Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1)) , nn.ReLU() , nn.Dropout2d(p=0.022897457910152297, inplace=False) , nn.Conv2d(27, 28, kernel_size=(5, 5), stride=(1, 1)) , nn.LeakyReLU(negative_slope=0.5719652455913541) , nn.MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False) , nn.Flatten() , nn.Linear(in_features=700, out_features=28, bias=True) , nn.LeakyReLU(negative_slope=0.48187737007312437) , nn.Dropout(p=0.3735548245474202, inplace=False) , nn.Linear(in_features=28, out_features=10, bias=True) , nn.BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) )\n",
    "\n",
    "\n",
    "input_size = 100\n",
    "hidden_sizes = [60, 30]\n",
    "output_size = 1\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n",
    "\n",
    "\n",
    "model_train = nn.Sequential(\n",
    "                      nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      )\n",
    "#nn.Softmax(dim=0)\n",
    "# Define the loss\n",
    "criterion = nn.MSELoss()\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = torch.optim.SGD(model_train.parameters(), lr=0.003)\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(1878):\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model_train(torch_tensor_train[i])\n",
    "        #print(output)\n",
    "        #torch.tensor(y_train[i])\n",
    "        loss = criterion(output, torch.tensor(y_train[i]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        \n",
    "model_val = nn.Sequential(\n",
    "                      nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      )\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = torch.optim.SGD(model_val.parameters(), lr=0.003)\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(1878):\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model_val(torch_tensor_val[i])\n",
    "        #print(output)\n",
    "        loss = criterion(output, torch.tensor(y_val[i]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "print(\"finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here!\n",
      "476\n",
      "[0.8363585472106934, 0.36869680881500244, 0.5994740724563599, 0.8328453302383423, 0.8060317039489746, 0.5057187080383301, 0.8639028668403625, 0.36450228095054626, 0.7888296842575073, 0.15077286958694458, 0.39467790722846985, 0.38780543208122253, -0.010000735521316528, 0.6006891131401062, 0.23683147132396698, 0.4370037019252777, 0.24581709504127502, 0.4277389645576477, 0.18133270740509033, 0.10450480878353119, 0.021694332361221313, 0.49506059288978577, 0.7593083381652832, 0.04006585478782654, 0.6338014006614685, 0.5114523768424988, 0.8090798854827881, 0.02938300371170044, 0.03021952509880066, 0.284918874502182, 0.861266553401947, 0.20946034789085388, 0.7754078507423401, 0.08231362700462341, 0.02386331558227539, 0.7157660126686096, 0.5392324328422546, 0.7932217121124268, 0.5845357179641724, 0.17960576713085175, 0.6044695377349854, 0.8017895221710205, 0.8576793074607849, 0.8555430173873901, 0.1606748253107071, 0.014012515544891357, 0.1951383352279663, 0.23873388767242432, 0.4450978636741638, 0.5690672993659973, 0.6849546432495117, 0.8402652144432068, 0.8508955240249634, 0.68915855884552, 0.582579493522644, 0.05446532368659973, 0.5108164548873901, 0.006649017333984375, 0.8492525815963745, 0.03964531421661377, 0.42051517963409424, 0.03718551993370056, 0.396129846572876, 0.08371908962726593, 0.6505871415138245, 0.03274163603782654, 0.694916844367981, 0.31201788783073425, 0.1659354716539383, 0.535895824432373, 0.24308687448501587, 0.2065780907869339, 0.34068360924720764, 0.585925817489624, 0.6541242599487305, 0.6787834167480469, 0.8299514055252075, 0.5832905769348145, 0.27703291177749634, 0.35241732001304626, 0.19117513298988342, 0.6839859485626221, 0.7067527174949646, 0.8611924648284912, 0.12144534289836884, 0.46641185879707336, 0.010632932186126709, 0.014661848545074463, 0.1824849545955658, 0.6841704845428467, 0.1876230686903, 0.33828669786453247, 0.8230839967727661, 0.31160861253738403, 0.2583906352519989, 0.8026716709136963, 0.21258561313152313, 0.5003441572189331, 0.8011577129364014, 0.09474378824234009, -0.008533328771591187, 0.33375436067581177, 0.2075985074043274, 0.5920418500900269, 0.2424633949995041, 0.8597434759140015, 0.07021081447601318, 0.490541934967041, 0.8503997921943665, 0.44357192516326904, 0.8430670499801636, 0.06601440906524658, 0.5193859934806824, 0.5487058162689209, 0.8605112433433533, 0.0616002082824707, 0.5401536822319031, 0.7723590135574341, 0.43723630905151367, 0.08216598629951477, 0.7110586166381836, 0.859774112701416, 0.012138187885284424, 0.7834868431091309, 0.3971555829048157, 0.6544681191444397, 0.8040147423744202, 0.3672851324081421, 0.7297425866127014, 0.33095693588256836, 0.6921660900115967, 0.3111298084259033, 0.26645955443382263, 0.2924448251724243, 0.6202939748764038, 0.7206336259841919, 0.45480766892433167, 0.8606047034263611, 0.20433352887630463, 0.81586754322052, 0.8134312629699707, 0.8533895015716553, 0.009801626205444336, 0.1258239597082138, 0.18313315510749817, 0.5402842164039612, 0.12729765474796295, 0.05761149525642395, 0.8096277117729187, 0.7365591526031494, 0.6097719669342041, 0.8277670741081238, 0.7424048185348511, 0.5764193534851074, 0.5453985929489136, 0.16929323971271515, 0.16360346972942352, 0.8625381588935852, 0.8430421948432922, 0.1089717298746109, 0.49158135056495667, 0.07580766081809998, 0.8057429194450378, -0.002477675676345825, 0.3583546578884125, 0.15420351922512054, 0.8519887328147888, 0.05236774682998657, -0.011193960905075073, 0.44946399331092834, 0.9700862169265747, 0.26895570755004883, 0.4413074851036072, 0.5592477321624756, 0.44981157779693604, 0.7353452444076538, 0.6729797124862671, 0.7071382999420166, 0.8215950131416321, 0.12026362121105194, 0.7679170370101929, 0.7003448605537415, 0.8626374006271362, 0.86208575963974, 0.4670588970184326, 0.861305832862854, 0.5072814226150513, 0.6307008266448975, 0.3313188850879669, 0.37480124831199646, 0.8566217422485352, 0.33917155861854553, 0.10118906199932098, 0.7948011159896851, 0.3593556880950928, 0.5426809787750244, 0.04713845252990723, 0.44262272119522095, 0.5297667980194092, 0.859041690826416, -0.0019765496253967285, 0.8905905485153198, 0.15923146903514862, 0.8594016432762146, 0.7934150695800781, 0.5846363306045532, 0.22812244296073914, 0.8751659989356995, 0.5656092166900635, 0.3046998083591461, 0.7560097575187683, 0.4203411936759949, 0.48917850852012634, 0.4979204535484314, 0.8328741192817688, 0.8429160714149475, 0.67156583070755, 0.3655565679073334, 0.8639044165611267, 0.2156015932559967, 0.8614477515220642, 0.7080403566360474, 0.2777143120765686, 0.007982909679412842, 0.2074466347694397, 0.6438344717025757, 0.6647559404373169, 0.6716748476028442, 0.39428821206092834, 0.4822860062122345, 0.6375045776367188, 0.3102509379386902, 0.8036506772041321, 0.6638585329055786, 0.8620202541351318, 0.15468046069145203, 0.05196055769920349, 0.6053097248077393, 0.368257075548172, 0.8567296862602234, 0.4192239046096802, 0.07488805055618286, 0.47046583890914917, 0.5478891730308533, 0.61668860912323, 0.7423563003540039, 0.5382887125015259, 0.08012491464614868, 0.8559476733207703, 0.14728614687919617, 0.2715069055557251, 0.7302762269973755, 0.7442834973335266, 0.7709896564483643, 0.3911861479282379, 0.023004144430160522, 0.13101351261138916, 0.6942173838615417, 0.21076655387878418, 0.015952974557876587, 0.798719048500061, 0.10006092488765717, 0.4782443344593048, 0.04575139284133911, 0.3807843327522278, 0.5449132919311523, 0.20843443274497986, 0.1734156459569931, 0.8068153858184814, 0.4620232582092285, 0.2967926263809204, 0.0766066312789917, 0.2777784466743469, 0.7953586578369141, 0.6543729305267334, 0.07437264919281006, 0.6215329170227051, 0.25184619426727295, 0.8614427447319031, 0.0854468047618866, -0.0077551305294036865, 0.18832480907440186, 0.5130549669265747, 0.23546114563941956, 0.4119604825973511, 0.41331589221954346, 0.5615365505218506, 0.11435286700725555, 0.8286316990852356, 0.19231370091438293, 0.34933656454086304, 0.09725204110145569, 0.5982425212860107, 0.6691441535949707, 0.7234100103378296, 0.592705249786377, 0.06182366609573364, 0.022652626037597656, 0.6435801982879639, 0.28321945667266846, -0.004333227872848511, 0.09605903923511505, 0.49403607845306396, 0.19513162970542908, 0.12169553339481354, 0.411828875541687, 0.8674979209899902, 0.6193671226501465, 0.32912564277648926, 0.32777833938598633, 0.6050140261650085, 0.8284684419631958, 0.2068146914243698, 0.7085226774215698, 0.7900750637054443, -0.014824658632278442, 0.7413398623466492, 0.3248063623905182, 0.8317358493804932, 0.6525763273239136, 0.3092307150363922, 0.5933321118354797, 0.29424571990966797, 0.5508748292922974, 0.07449862360954285, 0.8582242727279663, 0.8531450629234314, 0.8597699999809265, 0.3280804455280304, 0.36964625120162964, 0.653815746307373, 0.7954398989677429, 0.6927337646484375, 0.40613532066345215, 0.12546513974666595, -0.011565953493118286, 0.022867053747177124, 0.7222636938095093, 0.48130685091018677, 0.29395920038223267, 0.783684253692627, 0.41988813877105713, 0.23450417816638947, 0.8585999011993408, 0.6423088908195496, 0.29406529664993286, 0.7510206699371338, 0.7056877017021179, 0.3566495180130005, 0.16494090855121613, 0.37356022000312805, 0.6342779994010925, 0.23659074306488037, 0.48365217447280884, 0.6991083025932312, 0.4924279451370239, 0.8602587580680847, 0.7674802541732788, 0.0331244170665741, 0.8603181838989258, 0.21035635471343994, 0.026173144578933716, 0.6624547243118286, 0.8095386028289795, 0.08007121086120605, 0.49708324670791626, 0.553642988204956, 0.6925281286239624, 0.5173857808113098, 0.7515209913253784, 0.3619392514228821, 0.8600907325744629, 0.04562455415725708, 0.14177100360393524, 0.8054822087287903, 0.8542163968086243, 0.1738797426223755, 0.7970997095108032, 0.3670077323913574, 0.6149163246154785, 0.15942417085170746, 0.35916969180107117, 0.5689132213592529, 0.4112014174461365, 0.14011001586914062, 0.5965659022331238, 0.13809794187545776, 0.8666457533836365, 0.8634212613105774, 0.6796412467956543, 0.5949654579162598, 0.46948015689849854, 0.8615319132804871, -0.03917562961578369, -0.010483473539352417, 0.1748584359884262, 0.429515540599823, 0.23561763763427734, 0.580123782157898, 0.5433984994888306, -0.01290246844291687, 0.8583205938339233, 0.7609379291534424, 0.7875921726226807, 0.6023398637771606, 0.4430782198905945, 0.6606961488723755, 0.8384469747543335, 0.08277750015258789, 0.7747023701667786, 0.023324012756347656, 0.44418954849243164, 0.4342804551124573, 0.8643991351127625, 0.6686241626739502, -0.011689633131027222, 0.5948245525360107, 0.4329906105995178, 0.006007373332977295, 0.7978860139846802, 0.8671700358390808, 0.48785340785980225, 0.7610672116279602, 0.2470850944519043, 0.6687740087509155, 0.6615628004074097, 0.5756362080574036, 0.8233790993690491, 0.6307178735733032, 0.5048595666885376, 0.04561150074005127, -0.0036766529083251953, -0.02755841612815857, 0.21539051830768585, 0.8788544535636902, 0.8621180653572083, 0.2549948990345001, 0.38229501247406006, -0.015312343835830688, 0.3005334734916687, 0.19703780114650726, 0.8599573373794556, 0.2701580226421356, 0.5085424780845642, 0.3425261378288269, 0.5016147494316101, 0.0063006579875946045, 0.5424925088882446, 0.41237232089042664, 0.4395618140697479, 0.528934121131897, 0.8602428436279297, 0.0169450044631958, 0.4739578366279602, 0.3473977744579315, 0.8175632357597351, 0.34523725509643555, 0.4803794026374817, 0.0034477412700653076, -4.628300666809082e-05, 0.6895120739936829, 0.4910120368003845, 0.22912541031837463, 0.322192519903183, 0.6038988828659058, 0.47834479808807373, 0.6146624088287354, 0.39037808775901794, 0.2692318260669708, 0.6491284370422363, 0.30741408467292786, 0.05674225091934204, 0.7086342573165894, 0.04130592942237854, 0.3341790437698364, 0.42040953040122986]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#now we do the prediction\n",
    "\n",
    "print(\"I am here!\")\n",
    "res_train,res_val = [],[]\n",
    "\n",
    "for row in torch_tensor_test_train:\n",
    "    res_train.append(model_train(row).item())\n",
    "    \n",
    "for row in torch_tensor_test_val:\n",
    "    res_val.append(model_val(row).item())\n",
    "\n",
    "print(len(res_val))\n",
    "print(res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.53844213e-01  8.36358547e-01  4.04747903e-01  3.68696809e-01\n",
      "  5.22167146e-01  5.99474072e-01  6.73149347e-01  8.32845330e-01\n",
      "  5.95536530e-01  8.06031704e-01  5.40258586e-01  5.05718708e-01\n",
      "  7.17043161e-01  8.63902867e-01  4.23309684e-01  3.64502281e-01\n",
      "  6.61685050e-01  7.88829684e-01  3.28574777e-01  1.50772870e-01\n",
      "  4.71360892e-01  3.94677907e-01  3.98449183e-01  3.87805432e-01\n",
      "  3.54828089e-01 -1.00007355e-02  4.95136976e-01  6.00689113e-01\n",
      "  4.24124539e-01  2.36831471e-01  5.04078329e-01  4.37003702e-01\n",
      "  4.41792965e-01  2.45817095e-01  4.34599578e-01  4.27738965e-01\n",
      "  3.91934097e-01  1.81332707e-01  4.45642114e-01  1.04504809e-01\n",
      "  3.84893060e-01  2.16943324e-02  4.12839144e-01  4.95060593e-01\n",
      "  6.11738145e-01  7.59308338e-01  2.93177247e-01  4.00658548e-02\n",
      "  5.44147193e-01  6.33801401e-01  4.74901736e-01  5.11452377e-01\n",
      "  6.59819424e-01  8.09079885e-01  3.27300251e-01  2.93830037e-02\n",
      "  4.50783670e-01  3.02195251e-02  3.29030216e-01  2.84918875e-01\n",
      "  7.53093123e-01  8.61266553e-01  3.90172780e-01  2.09460348e-01\n",
      "  6.82244778e-01  7.75407851e-01  4.65993464e-01  8.23136270e-02\n",
      "  4.86707658e-01  2.38633156e-02  6.08678162e-01  7.15766013e-01\n",
      "  5.14583170e-01  5.39232433e-01  6.24831975e-01  7.93221712e-01\n",
      "  4.65483040e-01  5.84535718e-01  4.41098571e-01  1.79605767e-01\n",
      "  6.41400278e-01  6.04469538e-01  6.41898870e-01  8.01789522e-01\n",
      "  7.19336271e-01  8.57679307e-01  6.13228321e-01  8.55543017e-01\n",
      "  4.43566769e-01  1.60674825e-01  3.82270217e-01  1.40125155e-02\n",
      "  5.09570062e-01  1.95138335e-01  4.37538594e-01  2.38733888e-01\n",
      "  5.03174603e-01  4.45097864e-01  4.96093541e-01  5.69067299e-01\n",
      "  3.09750199e-01  6.84954643e-01  6.50073469e-01  8.40265214e-01\n",
      "  7.15767741e-01  8.50895524e-01  5.37631989e-01  6.89158559e-01\n",
      "  5.32105744e-01  5.82579494e-01  4.46768463e-01  5.44653237e-02\n",
      "  4.45509404e-01  5.10816455e-01  3.94621670e-01  6.64901733e-03\n",
      "  6.67516470e-01  8.49252582e-01  4.46394920e-01  3.96453142e-02\n",
      "  4.76255894e-01  4.20515180e-01  3.56339633e-01  3.71855199e-02\n",
      "  4.34396714e-01  3.96129847e-01  3.77210438e-01  8.37190896e-02\n",
      "  5.27472436e-01  6.50587142e-01  3.71017963e-01  3.27416360e-02\n",
      "  5.73497713e-01  6.94916844e-01  3.75636220e-01  3.12017888e-01\n",
      "  4.45839286e-01  1.65935472e-01  3.76533270e-01  5.35895824e-01\n",
      "  3.28455389e-01  2.43086874e-01  4.56412196e-01  2.06578091e-01\n",
      "  4.43932205e-01  3.40683609e-01  4.85671580e-01  5.85925817e-01\n",
      "  5.19981921e-01  6.54124260e-01  5.15673757e-01  6.78783417e-01\n",
      "  6.55612588e-01  8.29951406e-01  4.05788600e-01  5.83290577e-01\n",
      "  4.01322544e-01  2.77032912e-01  4.03179526e-01  3.52417320e-01\n",
      "  4.34295535e-01  1.91175133e-01  5.44765472e-01  6.83985949e-01\n",
      "  6.28779352e-01  7.06752717e-01  7.24232316e-01  8.61192465e-01\n",
      "  5.50971925e-01  1.21445343e-01  4.09098148e-01  4.66411859e-01\n",
      "  4.43570137e-01  1.06329322e-02  3.94315273e-01  1.46618485e-02\n",
      "  4.18056220e-01  1.82484955e-01  4.65623438e-01  6.84170485e-01\n",
      "  4.23897356e-01  1.87623069e-01  3.40548038e-01  3.38286698e-01\n",
      "  6.63011074e-01  8.23083997e-01  3.88294429e-01  3.11608613e-01\n",
      "  4.71198857e-01  2.58390635e-01  4.02786374e-01  8.02671671e-01\n",
      "  4.46629107e-01  2.12585613e-01  4.44846809e-01  5.00344157e-01\n",
      "  6.58967614e-01  8.01157713e-01  3.16711664e-01  9.47437882e-02\n",
      "  3.66745412e-01 -8.53332877e-03  4.27310914e-01  3.33754361e-01\n",
      "  4.60574150e-01  2.07598507e-01  5.01586974e-01  5.92041850e-01\n",
      "  5.51847696e-01  2.42463395e-01  7.19062209e-01  8.59743476e-01\n",
      "  3.85038257e-01  7.02108145e-02  6.84494078e-01  4.90541935e-01\n",
      "  6.68528557e-01  8.50399792e-01  4.33987647e-01  4.43571925e-01\n",
      "  5.34108937e-01  8.43067050e-01  4.54637945e-01  6.60144091e-02\n",
      "  5.01265585e-01  5.19385993e-01  4.81905162e-01  5.48705816e-01\n",
      "  7.27047503e-01  8.60511243e-01  4.36382115e-01  6.16002083e-02\n",
      "  4.36949134e-01  5.40153682e-01  6.56843126e-01  7.72359014e-01\n",
      "  4.38715965e-01  4.37236309e-01  4.61263806e-01  8.21659863e-02\n",
      "  4.36884314e-01  7.11058617e-01  7.26026535e-01  8.59774113e-01\n",
      "  3.33422363e-01  1.21381879e-02  6.44222856e-01  7.83486843e-01\n",
      "  4.60049957e-01  3.97155583e-01  5.52235186e-01  6.54468119e-01\n",
      "  3.40838015e-01  8.04014742e-01  4.18507814e-01  3.67285132e-01\n",
      "  5.66513240e-01  7.29742587e-01  4.96119261e-01  3.30956936e-01\n",
      "  5.34867287e-01  6.92166090e-01  4.49651390e-01  3.11129808e-01\n",
      "  4.70300078e-01  2.66459554e-01  4.37162280e-01  2.92444825e-01\n",
      "  5.48704207e-01  6.20293975e-01  6.20460927e-01  7.20633626e-01\n",
      "  4.19589341e-01  4.54807669e-01  7.24388599e-01  8.60604703e-01\n",
      "  4.44791377e-01  2.04333529e-01  6.23310804e-01  8.15867543e-01\n",
      "  5.90331614e-01  8.13431263e-01  6.60373032e-01  8.53389502e-01\n",
      "  4.67135429e-01  9.80162621e-03  4.14902031e-01  1.25823960e-01\n",
      "  3.61465335e-01  1.83133155e-01  4.78303194e-01  5.40284216e-01\n",
      "  4.81814116e-01  1.27297655e-01  3.74739438e-01  5.76114953e-02\n",
      "  7.21385598e-01  8.09627712e-01  6.12166822e-01  7.36559153e-01\n",
      "  5.10784566e-01  6.09771967e-01  5.88788092e-01  8.27767074e-01\n",
      "  5.96089542e-01  7.42404819e-01  5.38259685e-01  5.76419353e-01\n",
      "  5.52233458e-01  5.45398593e-01  4.53346372e-01  1.69293240e-01\n",
      "  4.50150907e-01  1.63603470e-01  7.25989819e-01  8.62538159e-01\n",
      "  7.06192315e-01  8.43042195e-01  3.73341084e-01  1.08971730e-01\n",
      "  5.17563879e-01  4.91581351e-01  3.96234959e-01  7.58076608e-02\n",
      "  3.88487130e-01  8.05742919e-01  3.06544542e-01 -2.47767568e-03\n",
      "  4.04380292e-01  3.58354658e-01  4.67824221e-01  1.54203519e-01\n",
      "  5.36989152e-01  8.51988733e-01  4.24893826e-01  5.23677468e-02\n",
      "  3.40285480e-01 -1.11939609e-02  4.37392890e-01  4.49463993e-01\n",
      "  7.09918201e-01  9.70086217e-01  4.06207800e-01  2.68955708e-01\n",
      "  4.50639546e-01  4.41307485e-01  5.08162916e-01  5.59247732e-01\n",
      "  4.60071713e-01  4.49811578e-01  5.20237803e-01  7.35345244e-01\n",
      "  5.35861731e-01  6.72979712e-01  5.12698472e-01  7.07138300e-01\n",
      "  6.47946537e-01  8.21595013e-01  3.53097886e-01  1.20263621e-01\n",
      "  6.68558836e-01  7.67917037e-01  5.94639301e-01  7.00344861e-01\n",
      "  6.66623056e-01  8.62637401e-01  7.16134965e-01  8.62085760e-01\n",
      "  3.28893721e-01  4.67058897e-01  7.22527206e-01  8.61305833e-01\n",
      "  6.16511285e-01  5.07281423e-01  4.17858541e-01  6.30700827e-01\n",
      "  4.26291972e-01  3.31318885e-01  3.94056320e-01  3.74801248e-01\n",
      "  7.03532159e-01  8.56621742e-01  3.35475594e-01  3.39171559e-01\n",
      "  4.50807452e-01  1.01189062e-01  6.62185431e-01  7.94801116e-01\n",
      "  5.13936937e-01  3.59355688e-01  4.62389410e-01  5.42680979e-01\n",
      "  3.86299878e-01  4.71384525e-02  5.21180809e-01  4.42622721e-01\n",
      "  4.82133716e-01  5.29766798e-01  6.98714852e-01  8.59041691e-01\n",
      "  4.62476403e-01 -1.97654963e-03  6.12384915e-01  8.90590549e-01\n",
      "  3.05232108e-01  1.59231469e-01  6.62549794e-01  8.59401643e-01\n",
      "  5.48577905e-01  7.93415070e-01  4.17248964e-01  5.84636331e-01\n",
      "  4.77024615e-01  2.28122443e-01  5.99423528e-01  8.75165999e-01\n",
      "  6.25806689e-01  5.65609217e-01  4.46873128e-01  3.04699808e-01\n",
      "  5.50880194e-01  7.56009758e-01  4.40607131e-01  4.20341194e-01\n",
      "  4.53466207e-01  4.89178509e-01  5.30249834e-01  4.97920454e-01\n",
      "  6.31491125e-01  8.32874119e-01  5.23070872e-01  8.42916071e-01\n",
      "  4.42156911e-01  6.71565831e-01  4.26280409e-01  3.65556568e-01\n",
      "  7.19731092e-01  8.63904417e-01  4.20315951e-01  2.15601593e-01\n",
      "  7.24272072e-01  8.61447752e-01  5.00370622e-01  7.08040357e-01\n",
      "  4.59095418e-01  2.77714312e-01  3.59758854e-01  7.98290968e-03\n",
      "  4.76880729e-01  2.07446635e-01  5.19885361e-01  6.43834472e-01\n",
      "  5.00371575e-01  6.64755940e-01  5.03433228e-01  6.71674848e-01\n",
      "  4.59924132e-01  3.94288212e-01  4.67955828e-01  4.82286006e-01\n",
      "  5.15198827e-01  6.37504578e-01  4.48352695e-01  3.10250938e-01\n",
      "  6.25507593e-01  8.03650677e-01  5.27301729e-01  6.63858533e-01\n",
      "  7.01005578e-01  8.62020254e-01  2.89311945e-01  1.54680461e-01\n",
      "  5.05029202e-01  5.19605577e-02  4.49430406e-01  6.05309725e-01\n",
      "  3.67157161e-01  3.68257076e-01  7.23046064e-01  8.56729686e-01\n",
      "  4.40361142e-01  4.19223905e-01  4.56655234e-01  7.48880506e-02\n",
      "  4.89477366e-01  4.70465839e-01  4.84153092e-01  5.47889173e-01\n",
      "  5.84147036e-01  6.16688609e-01  5.19524693e-01  7.42356300e-01\n",
      "  4.70780075e-01  5.38288713e-01  3.56809914e-01  8.01249146e-02\n",
      "  7.20551014e-01  8.55947673e-01  3.80582541e-01  1.47286147e-01\n",
      "  3.47349942e-01  2.71506906e-01  5.19831061e-01  7.30276227e-01\n",
      "  4.92166668e-01  7.44283497e-01  3.32978338e-01  7.70989656e-01\n",
      "  4.32393491e-01  3.91186148e-01  4.43508536e-01  2.30041444e-02\n",
      "  3.30639809e-01  1.31013513e-01  5.39769113e-01  6.94217384e-01\n",
      "  3.83165449e-01  2.10766554e-01  4.51646745e-01  1.59529746e-02\n",
      "  5.77230990e-01  7.98719049e-01  4.46987748e-01  1.00060925e-01\n",
      " -8.57305527e-02  4.78244334e-01  4.63236153e-01  4.57513928e-02\n",
      "  4.87487108e-01  3.80784333e-01  4.54261631e-01  5.44913292e-01\n",
      "  4.10452425e-01  2.08434433e-01  4.54350024e-01  1.73415646e-01\n",
      "  6.10408247e-01  8.06815386e-01  4.36256766e-01  4.62023258e-01\n",
      "  3.76361787e-01  2.96792626e-01  4.17312026e-01  7.66066313e-02\n",
      "  3.95370543e-01  2.77778447e-01  6.10643804e-01  7.95358658e-01\n",
      "  7.12118089e-01  6.54372931e-01  3.46522689e-01  7.43726492e-02\n",
      "  5.08787036e-01  6.21532917e-01  4.39355254e-01  2.51846194e-01\n",
      "  7.19806135e-01  8.61442745e-01  3.65586072e-01  8.54468048e-02\n",
      "  4.95813400e-01 -7.75513053e-03  4.67213690e-01  1.88324809e-01\n",
      "  5.81614137e-01  5.13054967e-01  3.31780732e-01  2.35461146e-01\n",
      "  4.34250265e-01  4.11960483e-01  4.30626929e-01  4.13315892e-01\n",
      "  3.37987900e-01  5.61536551e-01  4.60451007e-01  1.14352867e-01\n",
      "  6.03086472e-01  8.28631699e-01  4.63824868e-01  1.92313701e-01\n",
      "  4.51437384e-01  3.49336565e-01  3.96174431e-01  9.72520411e-02\n",
      "  4.15342003e-01  5.98242521e-01  5.44328451e-01  6.69144154e-01\n",
      "  4.70210969e-01  7.23410010e-01  5.12525201e-01  5.92705250e-01\n",
      "  2.98631370e-01  6.18236661e-02  4.30286944e-01  2.26526260e-02\n",
      "  5.39695382e-01  6.43580198e-01  4.34817255e-01  2.83219457e-01\n",
      "  4.13292408e-01 -4.33322787e-03  3.79536510e-01  9.60590392e-02\n",
      "  4.52507764e-01  4.94036078e-01  3.81146133e-01  1.95131630e-01\n",
      "  4.59581614e-01  1.21695533e-01  3.85107547e-01  4.11828876e-01\n",
      "  7.15711594e-01  8.67497921e-01  5.72974384e-01  6.19367123e-01\n",
      "  4.08178627e-01  3.29125643e-01  4.70709145e-01  3.27778339e-01\n",
      "  5.05841672e-01  6.05014026e-01  6.50563717e-01  8.28468442e-01\n",
      "  4.06960338e-01  2.06814691e-01  5.82742214e-01  7.08522677e-01\n",
      "  6.43133342e-01  7.90075064e-01  4.61390674e-01 -1.48246586e-02\n",
      "  4.58872467e-01  7.41339862e-01  4.34759468e-01  3.24806362e-01\n",
      "  6.27805173e-01  8.31735849e-01  5.87823689e-01  6.52576327e-01\n",
      "  4.33334172e-01  3.09230715e-01  6.76946640e-01  5.93332112e-01\n",
      "  4.48180377e-01  2.94245720e-01  5.00123262e-01  5.50874829e-01\n",
      "  4.80949759e-01  7.44986236e-02  7.17112660e-01  8.58224273e-01\n",
      "  6.87532187e-01  8.53145063e-01  7.26237774e-01  8.59770000e-01\n",
      "  4.21587884e-01  3.28080446e-01  4.05969232e-01  3.69646251e-01\n",
      "  5.29518306e-01  6.53815746e-01  6.33869827e-01  7.95439899e-01\n",
      "  5.91955781e-01  6.92733765e-01  4.98473823e-01  4.06135321e-01\n",
      "  4.81021047e-01  1.25465140e-01  4.39074427e-01 -1.15659535e-02\n",
      "  3.64489675e-01  2.28670537e-02  5.98954737e-01  7.22263694e-01\n",
      "  4.34541821e-01  4.81306851e-01  4.34827268e-01  2.93959200e-01\n",
      "  6.63985610e-01  7.83684254e-01  4.22267884e-01  4.19888139e-01\n",
      "  4.88271683e-01  2.34504178e-01  6.68526351e-01  8.58599901e-01\n",
      "  5.65082729e-01  6.42308891e-01  4.54119176e-01  2.94065297e-01\n",
      "  4.03672993e-01  7.51020670e-01  5.40740490e-01  7.05687702e-01\n",
      "  3.68707478e-01  3.56649518e-01  4.63753015e-01  1.64940909e-01\n",
      "  7.19042480e-01  3.73560220e-01  4.92153078e-01  6.34277999e-01\n",
      "  3.71767581e-01  2.36590743e-01  3.89379740e-01  4.83652174e-01\n",
      "  6.32442594e-01  6.99108303e-01  4.75850344e-01  4.92427945e-01\n",
      "  7.25416780e-01  8.60258758e-01  6.01990044e-01  7.67480254e-01\n",
      "  3.51369798e-01  3.31244171e-02  7.12035894e-01  8.60318184e-01\n",
      "  4.18402672e-01  2.10356355e-01  3.69661719e-01  2.61731446e-02\n",
      "  4.47244763e-01  6.62454724e-01  4.99320179e-01  8.09538603e-01\n",
      "  3.15149903e-01  8.00712109e-02  4.45553571e-01  4.97083247e-01\n",
      "  4.16263431e-01  5.53642988e-01  5.14165580e-01  6.92528129e-01\n",
      "  4.29478586e-01  5.17385781e-01  6.73899412e-01  7.51520991e-01\n",
      "  4.59600419e-01  3.61939251e-01  7.21794128e-01  8.60090733e-01\n",
      "  3.97744894e-01  4.56245542e-02  3.75125289e-01  1.41771004e-01\n",
      "  5.89040697e-01  8.05482209e-01  6.08739913e-01  8.54216397e-01\n",
      "  4.59653050e-01  1.73879743e-01  5.40481865e-01  7.97099710e-01\n",
      "  3.86070728e-01  3.67007732e-01  5.03160238e-01  6.14916325e-01\n",
      "  4.74671304e-01  1.59424171e-01  5.01710951e-01  3.59169692e-01\n",
      "  4.80694830e-01  5.68913221e-01  4.82320577e-01  4.11201417e-01\n",
      "  4.16359544e-01  1.40110016e-01  4.56856370e-01  5.96565902e-01\n",
      "  3.95063937e-01  1.38097942e-01  8.03799927e-01  8.66645753e-01\n",
      "  7.47166932e-01  8.63421261e-01  5.25495112e-01  6.79641247e-01\n",
      "  5.02554595e-01  5.94965458e-01  4.69569236e-01  4.69480157e-01\n",
      "  7.22458959e-01  8.61531913e-01  4.84305024e-01 -3.91756296e-02\n",
      "  3.86040032e-01 -1.04834735e-02  3.15935194e-01  1.74858436e-01\n",
      "  4.29310530e-01  4.29515541e-01  4.68644142e-01  2.35617638e-01\n",
      "  6.20388150e-01  5.80123782e-01  4.71635401e-01  5.43398499e-01\n",
      "  4.32445586e-01 -1.29024684e-02  6.89793706e-01  8.58320594e-01\n",
      "  6.04827523e-01  7.60937929e-01  5.65862596e-01  7.87592173e-01\n",
      "  4.68537956e-01  6.02339864e-01  5.24050951e-01  4.43078220e-01\n",
      "  4.56564963e-01  6.60696149e-01  6.97416604e-01  8.38446975e-01\n",
      "  4.16507155e-01  8.27775002e-02  6.32200122e-01  7.74702370e-01\n",
      "  4.53498214e-01  2.33240128e-02  4.37156200e-01  4.44189548e-01\n",
      "  5.24868011e-01  4.34280455e-01  7.18337715e-01  8.64399135e-01\n",
      "  4.72225368e-01  6.68624163e-01  3.30943614e-01 -1.16896331e-02\n",
      "  4.26799655e-01  5.94824553e-01  4.48293209e-01  4.32990611e-01\n",
      "  3.62084985e-01  6.00737333e-03  6.57310545e-01  7.97886014e-01\n",
      "  6.52500153e-01  8.67170036e-01  5.05255222e-01  4.87853408e-01\n",
      "  4.24052536e-01  7.61067212e-01  4.17252243e-01  2.47085094e-01\n",
      "  5.25973856e-01  6.68774009e-01  5.04301429e-01  6.61562800e-01\n",
      "  4.46674019e-01  5.75636208e-01  5.01777172e-01  8.23379099e-01\n",
      "  5.20779133e-01  6.30717874e-01  4.92734969e-01  5.04859567e-01\n",
      "  3.58329177e-01  4.56115007e-02  4.44178164e-01 -3.67665291e-03\n",
      "  4.35396135e-01 -2.75584161e-02  3.93995076e-01  2.15390518e-01\n",
      "  4.71313447e-01  8.78854454e-01  7.11510777e-01  8.62118065e-01\n",
      "  5.16730428e-01  2.54994899e-01  4.33993816e-01  3.82295012e-01\n",
      "  3.73015970e-01 -1.53123438e-02  4.33355808e-01  3.00533473e-01\n",
      "  4.51101303e-01  1.97037801e-01  7.26360500e-01  8.59957337e-01\n",
      "  4.30451304e-01  2.70158023e-01  4.72638130e-01  5.08542478e-01\n",
      "  4.26243961e-01  3.42526138e-01  4.36028600e-01  5.01614749e-01\n",
      "  5.97010434e-01  6.30065799e-03  3.87350559e-01  5.42492509e-01\n",
      "  4.63729888e-01  4.12372321e-01  4.44859833e-01  4.39561814e-01\n",
      "  4.48719054e-01  5.28934121e-01  7.19603539e-01  8.60242844e-01\n",
      "  3.80423784e-01  1.69450045e-02  4.51840997e-01  4.73957837e-01\n",
      "  4.27899182e-01  3.47397774e-01  6.48876369e-01  8.17563236e-01\n",
      "  4.38634664e-01  3.45237255e-01  4.23319161e-01  4.80379403e-01\n",
      "  5.37126958e-01  3.44774127e-03  3.29255819e-01 -4.62830067e-05\n",
      "  5.42858303e-01  6.89512074e-01  4.19699848e-01  4.91012037e-01\n",
      "  3.48296732e-01  2.29125410e-01  4.49933171e-01  3.22192520e-01\n",
      "  5.13281643e-01  6.03898883e-01  4.87074912e-01  4.78344798e-01\n",
      "  4.53871131e-01  6.14662409e-01  4.49212998e-01  3.90378088e-01\n",
      "  3.96486849e-01  2.69231826e-01  6.91281676e-01  6.49128437e-01\n",
      "  4.68957365e-01  3.07414085e-01  3.67383718e-01  5.67422509e-02\n",
      "  5.20563841e-01  7.08634257e-01  3.89347017e-01  4.13059294e-02\n",
      "  4.54124510e-01  3.34179044e-01  4.72953945e-01  4.20409530e-01]\n"
     ]
    }
   ],
   "source": [
    "submit = pd.read_csv(\"sample_submission.csv\", index_col = 0)\n",
    "res_array = np.zeros(len(res_train)*2)\n",
    "\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while True:\n",
    "    \n",
    "    if i == len(res_train):\n",
    "        break\n",
    "    \n",
    "    res_array[j] = res_val[i]\n",
    "    j += 1\n",
    "    res_array[j] = res_train[i]\n",
    "    j += 1\n",
    "    \n",
    "    i += 1 \n",
    "\n",
    "print(res_array)\n",
    "\n",
    "submit.Predicted = res_array\n",
    "submit.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
