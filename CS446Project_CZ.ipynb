{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                        arch_and_hp  \\\n",
      "0  train_0  Sequential( (conv0): Conv2d(3, 28, kernel_size...   \n",
      "1  train_1  Sequential( (batchnorm0): BatchNorm2d(3, eps=1...   \n",
      "2  train_2  Sequential( (batchnorm0): BatchNorm2d(3, eps=1...   \n",
      "3  train_3  Sequential( (conv0): Conv2d(3, 32, kernel_size...   \n",
      "4  train_4  Sequential( (batchnorm0): BatchNorm2d(3, eps=1...   \n",
      "\n",
      "   batch_size_test  batch_size_val           criterion  epochs  \\\n",
      "0             1024             512  CrossEntropyLoss()     590   \n",
      "1             1024             512  CrossEntropyLoss()     556   \n",
      "2             1024             512  CrossEntropyLoss()     556   \n",
      "3             1024             512  CrossEntropyLoss()     550   \n",
      "4             1024             512  CrossEntropyLoss()     404   \n",
      "\n",
      "   number_parameters                                          optimizer  \\\n",
      "0             833577  Adam ( Parameter Group 0 amsgrad: False betas:...   \n",
      "1              55687  Adam ( Parameter Group 0 amsgrad: False betas:...   \n",
      "2             926455  Adam ( Parameter Group 0 amsgrad: False betas:...   \n",
      "3             873224  Adam ( Parameter Group 0 amsgrad: False betas:...   \n",
      "4             106291  Adam ( Parameter Group 0 amsgrad: False betas:...   \n",
      "\n",
      "   val_error  val_loss       ...         train_losses_40  train_losses_41  \\\n",
      "0   0.663281  1.804554       ...                2.273316         2.273133   \n",
      "1   0.344922  1.171982       ...                1.923573         1.920186   \n",
      "2   0.452734  2.991273       ...                0.947629         0.952411   \n",
      "3   0.417578  1.584186       ...                1.773677         1.770966   \n",
      "4   0.547656  1.497717       ...                2.063500         2.063436   \n",
      "\n",
      "   train_losses_42 train_losses_43 train_losses_44 train_losses_45  \\\n",
      "0         2.273273        2.273123        2.273181        2.273089   \n",
      "1         1.916200        1.917743        1.911381        1.919590   \n",
      "2         0.935966        0.925001        0.918707        0.914364   \n",
      "3         1.768318        1.765915        1.764277        1.759141   \n",
      "4         2.059886        2.061260        2.059897        2.061916   \n",
      "\n",
      "   train_losses_46  train_losses_47  train_losses_48  train_losses_49  \n",
      "0         2.273409         2.272900         2.273333         2.273214  \n",
      "1         1.919531         1.917131         1.908027         1.913393  \n",
      "2         0.898191         0.903883         0.891139         0.883376  \n",
      "3         1.758098         1.755463         1.752970         1.750570  \n",
      "4         2.060802         2.061664         2.055201         2.056667  \n",
      "\n",
      "[5 rows x 216 columns]\n",
      "              id                                        arch_and_hp  \\\n",
      "1873  train_1873  Sequential( (batchnorm0): BatchNorm2d(3, eps=1...   \n",
      "1874  train_1874  Sequential( (conv0): Conv2d(3, 30, kernel_size...   \n",
      "1875  train_1875  Sequential( (batchnorm0): BatchNorm2d(3, eps=1...   \n",
      "1876  train_1876  Sequential( (selu0): SELU() (softmax1): Softma...   \n",
      "1877  train_1877  Sequential( (flatten0): Flatten() (linear1): L...   \n",
      "\n",
      "      batch_size_test  batch_size_val           criterion  epochs  \\\n",
      "1873             1024             512  CrossEntropyLoss()     547   \n",
      "1874             1024             512  CrossEntropyLoss()     504   \n",
      "1875             1024             512  CrossEntropyLoss()     547   \n",
      "1876             1024             512  CrossEntropyLoss()     604   \n",
      "1877             1024             512  CrossEntropyLoss()     403   \n",
      "\n",
      "      number_parameters                                          optimizer  \\\n",
      "1873             977212  Adam ( Parameter Group 0 amsgrad: False betas:...   \n",
      "1874             891389  Adam ( Parameter Group 0 amsgrad: False betas:...   \n",
      "1875             106746  Adam ( Parameter Group 0 amsgrad: False betas:...   \n",
      "1876             804473  Adam ( Parameter Group 0 amsgrad: False betas:...   \n",
      "1877             102459  Adam ( Parameter Group 0 amsgrad: False betas:...   \n",
      "\n",
      "      val_error  val_loss       ...         train_losses_40  train_losses_41  \\\n",
      "1873   0.573047  1.564757       ...                2.152688         2.148418   \n",
      "1874   0.522656  5.078218       ...                1.368240         1.352972   \n",
      "1875   0.357812  1.227895       ...                1.433645         1.425469   \n",
      "1876   0.522266  3.059550       ...                1.274945         1.271599   \n",
      "1877   0.581641  1.569872       ...                2.122256         2.122341   \n",
      "\n",
      "      train_losses_42 train_losses_43 train_losses_44 train_losses_45  \\\n",
      "1873         2.146360        2.147887        2.148844        2.146595   \n",
      "1874         1.345273        1.344090        1.326210        1.331296   \n",
      "1875         1.425259        1.421844        1.421157        1.419637   \n",
      "1876         1.264117        1.255945        1.251034        1.246117   \n",
      "1877         2.116721        2.119756        2.113414        2.114638   \n",
      "\n",
      "      train_losses_46  train_losses_47  train_losses_48  train_losses_49  \n",
      "1873         2.146367         2.141046         2.139806         2.139841  \n",
      "1874         1.310993         1.296605         1.294416         1.283409  \n",
      "1875         1.415964         1.411966         1.406813         1.403312  \n",
      "1876         1.242710         1.234381         1.227466         1.221015  \n",
      "1877         2.114677         2.113445         2.117864         2.120155  \n",
      "\n",
      "[5 rows x 216 columns]\n",
      "0    Sequential( (conv0): Conv2d(3, 28, kernel_size...\n",
      "1    Sequential( (batchnorm0): BatchNorm2d(3, eps=1...\n",
      "2    Sequential( (batchnorm0): BatchNorm2d(3, eps=1...\n",
      "3    Sequential( (conv0): Conv2d(3, 32, kernel_size...\n",
      "4    Sequential( (batchnorm0): BatchNorm2d(3, eps=1...\n",
      "Name: arch_and_hp, dtype: object\n"
     ]
    }
   ],
   "source": [
    "my_data = pd.read_csv(\"train.csv\", index_col = 0)\n",
    "\n",
    "print(my_data.head())\n",
    "print(my_data.tail())\n",
    "print(my_data['arch_and_hp'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                        arch_and_hp  epochs  \\\n",
      "0  train_0  Sequential( (conv0): Conv2d(3, 28, kernel_size...     590   \n",
      "1  train_1  Sequential( (batchnorm0): BatchNorm2d(3, eps=1...     556   \n",
      "2  train_2  Sequential( (batchnorm0): BatchNorm2d(3, eps=1...     556   \n",
      "3  train_3  Sequential( (conv0): Conv2d(3, 32, kernel_size...     550   \n",
      "4  train_4  Sequential( (batchnorm0): BatchNorm2d(3, eps=1...     404   \n",
      "\n",
      "   number_parameters  val_loss  train_loss  \\\n",
      "0             833577  1.804554    2.271728   \n",
      "1              55687  1.171982    1.851642   \n",
      "2             926455  2.991273    0.183252   \n",
      "3             873224  1.584186    1.572950   \n",
      "4             106291  1.497717    2.010671   \n",
      "\n",
      "                                      init_params_mu  \\\n",
      "0  [0.014136802405118942, 0.05712495744228363, -0...   \n",
      "1  [0.4259982109069824, 0.026873130351305008, -0....   \n",
      "2  [1.047141432762146, 0.5885080099105835, -0.226...   \n",
      "3  [0.005588707514107227, -0.04767268896102905, 0...   \n",
      "4  [0.05355146527290344, -0.0010410345857962966, ...   \n",
      "\n",
      "                                     init_params_std  \\\n",
      "0  [0.34725773334503174, 0.34454283118247986, 0.2...   \n",
      "1  [0.08296086639165878, 0.07182817906141281, 0.4...   \n",
      "2  [0.37009814381599426, 0.1420908421278, 1.26942...   \n",
      "3  [0.35991349816322327, 0.05571247264742851, 0.2...   \n",
      "4  [0.008598395623266697, 0.006537274457514286, 0...   \n",
      "\n",
      "                                      init_params_l2  val_accs_0  \\\n",
      "0  [3.16632080078125, 1.815636157989502, 193.3981...    0.292188   \n",
      "1  [0.7471200823783875, 0.11173661798238754, 3.94...    0.271875   \n",
      "2  [1.887713074684143, 1.0389440059661865, 11.960...    0.282422   \n",
      "3  [3.508430242538452, 0.41103067994117737, 20.85...    0.298438   \n",
      "4  [0.09354753792285919, 0.00941929779946804, 47....    0.281250   \n",
      "\n",
      "        ...         train_losses_40  train_losses_41  train_losses_42  \\\n",
      "0       ...                2.273316         2.273133         2.273273   \n",
      "1       ...                1.923573         1.920186         1.916200   \n",
      "2       ...                0.947629         0.952411         0.935966   \n",
      "3       ...                1.773677         1.770966         1.768318   \n",
      "4       ...                2.063500         2.063436         2.059886   \n",
      "\n",
      "   train_losses_43  train_losses_44  train_losses_45  train_losses_46  \\\n",
      "0         2.273123         2.273181         2.273089         2.273409   \n",
      "1         1.917743         1.911381         1.919590         1.919531   \n",
      "2         0.925001         0.918707         0.914364         0.898191   \n",
      "3         1.765915         1.764277         1.759141         1.758098   \n",
      "4         2.061260         2.059897         2.061916         2.060802   \n",
      "\n",
      "   train_losses_47  train_losses_48  train_losses_49  \n",
      "0         2.272900         2.273333         2.273214  \n",
      "1         1.917131         1.908027         1.913393  \n",
      "2         0.903883         0.891139         0.883376  \n",
      "3         1.755463         1.752970         1.750570  \n",
      "4         2.061664         2.055201         2.056667  \n",
      "\n",
      "[5 rows x 209 columns]\n"
     ]
    }
   ],
   "source": [
    "my_data = my_data.drop(columns = ['batch_size_test', 'batch_size_val', 'criterion', 'optimizer', 'batch_size_train'])\n",
    "y_train = my_data['train_error']\n",
    "y_val = my_data['val_error']\n",
    "my_data = my_data.drop(columns = 'train_error')\n",
    "my_data = my_data.drop(columns = 'val_error')\n",
    "#my_data = my_data.to_numpy()\n",
    "\n",
    "print(my_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  epochs  number_parameters  val_accs_0  val_accs_1  val_accs_2  \\\n",
      "0  train_0     590             833577    0.292188    0.348047    0.376172   \n",
      "1  train_1     556              55687    0.271875    0.529687    0.553906   \n",
      "2  train_2     556             926455    0.282422    0.534375    0.554297   \n",
      "3  train_3     550             873224    0.298438    0.498047    0.533203   \n",
      "4  train_4     404             106291    0.281250    0.409766    0.404297   \n",
      "\n",
      "   val_accs_3  val_accs_4  val_accs_5  val_accs_6       ...         \\\n",
      "0    0.374219    0.381250    0.357422    0.349609       ...          \n",
      "1    0.573438    0.576172    0.589063    0.582812       ...          \n",
      "2    0.584766    0.574609    0.583594    0.585156       ...          \n",
      "3    0.559375    0.560156    0.578125    0.578516       ...          \n",
      "4    0.408594    0.420703    0.420703    0.440625       ...          \n",
      "\n",
      "   train_losses_40  train_losses_41  train_losses_42  train_losses_43  \\\n",
      "0         2.273316         2.273133         2.273273         2.273123   \n",
      "1         1.923573         1.920186         1.916200         1.917743   \n",
      "2         0.947629         0.952411         0.935966         0.925001   \n",
      "3         1.773677         1.770966         1.768318         1.765915   \n",
      "4         2.063500         2.063436         2.059886         2.061260   \n",
      "\n",
      "   train_losses_44  train_losses_45  train_losses_46  train_losses_47  \\\n",
      "0         2.273181         2.273089         2.273409         2.272900   \n",
      "1         1.911381         1.919590         1.919531         1.917131   \n",
      "2         0.918707         0.914364         0.898191         0.903883   \n",
      "3         1.764277         1.759141         1.758098         1.755463   \n",
      "4         2.059897         2.061916         2.060802         2.061664   \n",
      "\n",
      "   train_losses_48  train_losses_49  \n",
      "0         2.273333         2.273214  \n",
      "1         1.908027         1.913393  \n",
      "2         0.891139         0.883376  \n",
      "3         1.752970         1.750570  \n",
      "4         2.055201         2.056667  \n",
      "\n",
      "[5 rows x 203 columns]\n"
     ]
    }
   ],
   "source": [
    "x = my_data.drop(columns = ['train_loss','val_loss','arch_and_hp','init_params_mu', 'init_params_std', 'init_params_l2'])\n",
    "print(x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_train = [col for col in x.filter(regex='^train',axis=1).columns]\n",
    "col_val = [col for col in x.filter(regex='^val',axis=1).columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.drop(['id'],axis=1)\n",
    "train = x[col_train]\n",
    "val = x[col_val]\n",
    "\n",
    "test_data = pd.read_csv(\"test.csv\", index_col = 0)\n",
    "test_data = test_data.drop(columns = ['batch_size_test', 'batch_size_val', 'criterion', 'optimizer', 'batch_size_train'])\n",
    "test_data = test_data.drop(columns = ['arch_and_hp','init_params_mu', 'init_params_std', 'init_params_l2'])\n",
    "test_train = test_data[col_train]\n",
    "test_val = test_data[col_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtrain = xgb.DMatrix(train.values, y_train.values)\n",
    "xgtest = xgb.DMatrix(test_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'objective':'reg:squarederror' }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, xgtrain, num_round)\n",
    "# make prediction\n",
    "preds_train = bst.predict(xgtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88462734 0.33787948 0.576506   0.88462734 0.88462734 0.576506\n",
      " 0.88462734 0.33787948 0.8140439  0.09674081 0.33787948 0.33787948\n",
      " 0.01093802 0.576506   0.33787948 0.33787948 0.33787948 0.33787948\n",
      " 0.33787948 0.09674081 0.01093802 0.576506   0.8140439  0.01093802\n",
      " 0.576506   0.576506   0.8140439  0.01093802 0.03767467 0.33787948\n",
      " 0.88462734 0.09674081 0.8140439  0.09674081 0.09674081 0.8140439\n",
      " 0.576506   0.8140439  0.576506   0.09674081 0.576506   0.8140439\n",
      " 0.88462734 0.88462734 0.09674081 0.01093802 0.33787948 0.09674081\n",
      " 0.33787948 0.576506   0.8140439  0.88462734 0.88462734 0.8140439\n",
      " 0.576506   0.09674081 0.576506   0.01093802 0.88462734 0.03767467\n",
      " 0.33787948 0.03767467 0.33787948 0.09674081 0.576506   0.01093802\n",
      " 0.576506   0.33787948 0.09674081 0.576506   0.33787948 0.09674081\n",
      " 0.33787948 0.576506   0.576506   0.576506   0.88462734 0.576506\n",
      " 0.33787948 0.33787948 0.09674081 0.576506   0.8140439  0.88462734\n",
      " 0.33787948 0.576506   0.03767467 0.01093802 0.09674081 0.8140439\n",
      " 0.33787948 0.33787948 0.88462734 0.33787948 0.33787948 0.8140439\n",
      " 0.09674081 0.576506   0.8140439  0.09674081 0.01093802 0.33787948\n",
      " 0.09674081 0.576506   0.33787948 0.88462734 0.09674081 0.576506\n",
      " 0.88462734 0.576506   0.88462734 0.09674081 0.576506   0.576506\n",
      " 0.88462734 0.09674081 0.576506   0.8140439  0.33787948 0.09674081\n",
      " 0.8140439  0.88462734 0.01093802 0.8140439  0.33787948 0.576506\n",
      " 0.8140439  0.33787948 0.8140439  0.33787948 0.8140439  0.33787948\n",
      " 0.33787948 0.33787948 0.576506   0.8140439  0.576506   0.88462734\n",
      " 0.09674081 0.8140439  0.8140439  0.88462734 0.01093802 0.09674081\n",
      " 0.09674081 0.576506   0.09674081 0.09674081 0.8140439  0.8140439\n",
      " 0.576506   0.8140439  0.8140439  0.576506   0.576506   0.33787948\n",
      " 0.09674081 0.88462734 0.88462734 0.09674081 0.576506   0.09674081\n",
      " 0.8140439  0.01093802 0.33787948 0.09674081 0.88462734 0.09674081\n",
      " 0.01093802 0.576506   0.88462734 0.33787948 0.576506   0.576506\n",
      " 0.33787948 0.8140439  0.8140439  0.8140439  0.8140439  0.09674081\n",
      " 0.8140439  0.8140439  0.88462734 0.88462734 0.576506   0.88462734\n",
      " 0.576506   0.576506   0.33787948 0.33787948 0.88462734 0.33787948\n",
      " 0.09674081 0.8140439  0.33787948 0.576506   0.03767467 0.33787948\n",
      " 0.576506   0.88462734 0.01093802 0.88462734 0.09674081 0.88462734\n",
      " 0.8140439  0.576506   0.33787948 0.88462734 0.576506   0.33787948\n",
      " 0.8140439  0.33787948 0.576506   0.576506   0.88462734 0.88462734\n",
      " 0.8140439  0.33787948 0.88462734 0.33787948 0.88462734 0.8140439\n",
      " 0.33787948 0.01093802 0.09674081 0.576506   0.8140439  0.8140439\n",
      " 0.33787948 0.576506   0.576506   0.33787948 0.8140439  0.8140439\n",
      " 0.88462734 0.09674081 0.09674081 0.576506   0.33787948 0.88462734\n",
      " 0.33787948 0.09674081 0.576506   0.576506   0.576506   0.8140439\n",
      " 0.576506   0.09674081 0.88462734 0.09674081 0.33787948 0.8140439\n",
      " 0.8140439  0.8140439  0.33787948 0.03767467 0.09674081 0.8140439\n",
      " 0.33787948 0.01093802 0.8140439  0.09674081 0.576506   0.03767467\n",
      " 0.33787948 0.576506   0.09674081 0.09674081 0.8140439  0.576506\n",
      " 0.33787948 0.33787948 0.33787948 0.8140439  0.8140439  0.09674081\n",
      " 0.576506   0.33787948 0.88462734 0.09674081 0.01093802 0.33787948\n",
      " 0.576506   0.33787948 0.33787948 0.33787948 0.576506   0.09674081\n",
      " 0.88462734 0.09674081 0.33787948 0.09674081 0.576506   0.576506\n",
      " 0.8140439  0.576506   0.03767467 0.01093802 0.576506   0.33787948\n",
      " 0.01093802 0.09674081 0.576506   0.33787948 0.33787948 0.33787948\n",
      " 0.88462734 0.576506   0.33787948 0.33787948 0.576506   0.88462734\n",
      " 0.33787948 0.576506   0.8140439  0.01093802 0.8140439  0.33787948\n",
      " 0.88462734 0.576506   0.33787948 0.576506   0.33787948 0.576506\n",
      " 0.09674081 0.88462734 0.88462734 0.88462734 0.33787948 0.33787948\n",
      " 0.576506   0.8140439  0.8140439  0.33787948 0.09674081 0.01093802\n",
      " 0.01093802 0.8140439  0.576506   0.33787948 0.8140439  0.576506\n",
      " 0.33787948 0.88462734 0.576506   0.33787948 0.8140439  0.8140439\n",
      " 0.33787948 0.33787948 0.33787948 0.576506   0.33787948 0.576506\n",
      " 0.8140439  0.576506   0.88462734 0.8140439  0.09674081 0.88462734\n",
      " 0.09674081 0.01093802 0.576506   0.8140439  0.03767467 0.576506\n",
      " 0.576506   0.8140439  0.576506   0.8140439  0.33787948 0.88462734\n",
      " 0.09674081 0.09674081 0.8140439  0.88462734 0.09674081 0.8140439\n",
      " 0.33787948 0.576506   0.09674081 0.33787948 0.576506   0.576506\n",
      " 0.09674081 0.576506   0.09674081 0.88462734 0.88462734 0.8140439\n",
      " 0.576506   0.576506   0.88462734 0.09674081 0.01093802 0.09674081\n",
      " 0.33787948 0.33787948 0.576506   0.576506   0.01093802 0.88462734\n",
      " 0.8140439  0.8140439  0.576506   0.576506   0.8140439  0.88462734\n",
      " 0.09674081 0.8140439  0.03767467 0.33787948 0.576506   0.88462734\n",
      " 0.576506   0.01093802 0.576506   0.33787948 0.01093802 0.8140439\n",
      " 0.88462734 0.576506   0.8140439  0.33787948 0.8140439  0.576506\n",
      " 0.576506   0.88462734 0.576506   0.576506   0.09674081 0.01093802\n",
      " 0.01093802 0.09674081 0.88462734 0.88462734 0.33787948 0.33787948\n",
      " 0.01093802 0.33787948 0.09674081 0.88462734 0.33787948 0.576506\n",
      " 0.33787948 0.576506   0.03767467 0.576506   0.33787948 0.576506\n",
      " 0.576506   0.88462734 0.01093802 0.576506   0.33787948 0.8140439\n",
      " 0.33787948 0.576506   0.03767467 0.01093802 0.8140439  0.576506\n",
      " 0.09674081 0.33787948 0.576506   0.576506   0.576506   0.33787948\n",
      " 0.33787948 0.576506   0.33787948 0.03767467 0.8140439  0.09674081\n",
      " 0.33787948 0.33787948]\n"
     ]
    }
   ],
   "source": [
    "print(preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtrain = xgb.DMatrix(val.values, y_val.values)\n",
    "xgtest = xgb.DMatrix(test_val.values)\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'objective':'reg:squarederror' }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, xgtrain, num_round)\n",
    "# make prediction\n",
    "preds_val = bst.predict(xgtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5615234  0.4223884  0.4503824  0.69158584 0.5615234  0.5615234\n",
      " 0.69158584 0.44767267 0.5615234  0.3602188  0.4223884  0.3602188\n",
      " 0.3602188  0.49634358 0.4223884  0.49634358 0.44767267 0.4223884\n",
      " 0.3602188  0.44767267 0.3602188  0.4223884  0.5615234  0.3602188\n",
      " 0.6074846  0.4503824  0.69158584 0.3602188  0.44767267 0.33493453\n",
      " 0.69158584 0.3602188  0.69158584 0.44767267 0.4223884  0.69158584\n",
      " 0.4503824  0.5615234  0.4503824  0.4223884  0.69158584 0.5615234\n",
      " 0.69158584 0.5615234  0.44767267 0.3602188  0.4503824  0.4223884\n",
      " 0.49634358 0.4503824  0.33493453 0.69158584 0.69158584 0.5615234\n",
      " 0.5615234  0.44767267 0.4223884  0.3602188  0.69158584 0.44767267\n",
      " 0.49634358 0.3602188  0.4223884  0.3602188  0.5615234  0.3602188\n",
      " 0.5615234  0.33493453 0.44767267 0.33493453 0.3602188  0.44767267\n",
      " 0.44767267 0.5615234  0.5615234  0.4503824  0.69158584 0.4223884\n",
      " 0.4223884  0.33493453 0.44767267 0.5615234  0.69158584 0.69158584\n",
      " 0.6074846  0.4223884  0.44767267 0.3602188  0.4223884  0.4503824\n",
      " 0.44767267 0.3602188  0.69158584 0.4223884  0.4223884  0.4223884\n",
      " 0.44767267 0.4223884  0.69158584 0.3602188  0.3602188  0.4223884\n",
      " 0.44767267 0.4503824  0.5615234  0.69158584 0.3602188  0.5615234\n",
      " 0.69158584 0.4223884  0.5615234  0.44767267 0.4503824  0.4503824\n",
      " 0.69158584 0.4223884  0.4223884  0.69158584 0.4223884  0.44767267\n",
      " 0.4223884  0.69158584 0.3602188  0.69158584 0.49634358 0.5615234\n",
      " 0.33493453 0.4223884  0.5615234  0.4503824  0.5615234  0.44767267\n",
      " 0.49634358 0.4223884  0.5615234  0.5615234  0.4223884  0.69158584\n",
      " 0.44767267 0.5615234  0.5615234  0.69158584 0.44767267 0.4223884\n",
      " 0.3602188  0.4223884  0.44767267 0.3602188  0.69158584 0.5615234\n",
      " 0.5615234  0.5615234  0.5615234  0.5615234  0.5615234  0.44767267\n",
      " 0.44767267 0.69158584 0.69158584 0.3602188  0.5615234  0.3602188\n",
      " 0.4223884  0.3602188  0.4223884  0.44767267 0.5615234  0.3602188\n",
      " 0.3602188  0.4223884  0.69158584 0.4223884  0.44767267 0.5615234\n",
      " 0.4503824  0.5615234  0.5615234  0.5615234  0.69158584 0.3602188\n",
      " 0.69158584 0.5615234  0.69158584 0.69158584 0.33493453 0.69158584\n",
      " 0.5615234  0.4223884  0.4223884  0.33493453 0.69158584 0.3602188\n",
      " 0.4223884  0.69158584 0.4503824  0.4223884  0.3602188  0.4503824\n",
      " 0.4503824  0.69158584 0.44767267 0.5615234  0.3602188  0.69158584\n",
      " 0.5615234  0.4223884  0.44767267 0.5615234  0.5615234  0.4223884\n",
      " 0.5615234  0.4223884  0.4223884  0.5615234  0.69158584 0.4503824\n",
      " 0.4223884  0.44767267 0.69158584 0.3602188  0.69158584 0.4503824\n",
      " 0.44767267 0.3602188  0.44767267 0.5615234  0.5615234  0.4503824\n",
      " 0.49634358 0.4223884  0.4503824  0.4223884  0.5615234  0.5615234\n",
      " 0.69158584 0.3602188  0.44767267 0.4223884  0.33493453 0.69158584\n",
      " 0.4223884  0.44767267 0.49634358 0.4503824  0.5615234  0.5615234\n",
      " 0.4503824  0.3602188  0.69158584 0.3602188  0.33493453 0.5615234\n",
      " 0.4503824  0.33493453 0.44767267 0.44767267 0.3602188  0.5615234\n",
      " 0.3602188  0.44767267 0.5615234  0.44767267 0.69158584 0.44767267\n",
      " 0.69158584 0.4503824  0.4223884  0.44767267 0.5615234  0.4223884\n",
      " 0.3602188  0.3602188  0.3602188  0.5615234  0.69158584 0.3602188\n",
      " 0.4503824  0.4223884  0.69158584 0.3602188  0.49634358 0.4503824\n",
      " 0.5615234  0.3602188  0.4223884  0.44767267 0.33493453 0.44767267\n",
      " 0.5615234  0.49634358 0.4223884  0.3602188  0.4223884  0.5615234\n",
      " 0.4503824  0.5615234  0.3602188  0.44767267 0.5615234  0.44767267\n",
      " 0.44767267 0.3602188  0.4223884  0.3602188  0.44767267 0.33493453\n",
      " 0.69158584 0.5615234  0.4223884  0.49634358 0.5615234  0.5615234\n",
      " 0.3602188  0.5615234  0.5615234  0.44767267 0.49634358 0.44767267\n",
      " 0.5615234  0.69158584 0.3602188  0.69158584 0.44767267 0.4503824\n",
      " 0.44767267 0.69158584 0.69158584 0.69158584 0.4223884  0.4223884\n",
      " 0.5615234  0.5615234  0.5615234  0.4503824  0.44767267 0.3602188\n",
      " 0.3602188  0.5615234  0.4223884  0.4223884  0.69158584 0.4223884\n",
      " 0.44767267 0.69158584 0.5615234  0.4223884  0.4223884  0.5615234\n",
      " 0.3602188  0.44767267 0.69158584 0.4503824  0.3602188  0.33493453\n",
      " 0.5615234  0.4503824  0.69158584 0.5615234  0.33493453 0.69158584\n",
      " 0.3602188  0.3602188  0.4223884  0.4503824  0.3602188  0.4223884\n",
      " 0.4503824  0.5615234  0.4223884  0.69158584 0.4223884  0.69158584\n",
      " 0.3602188  0.33493453 0.5615234  0.5615234  0.44767267 0.5615234\n",
      " 0.3602188  0.4503824  0.44767267 0.5615234  0.4503824  0.49634358\n",
      " 0.3602188  0.4503824  0.3602188  0.69158584 0.69158584 0.5615234\n",
      " 0.4503824  0.49634358 0.69158584 0.49634358 0.3602188  0.3602188\n",
      " 0.4223884  0.44767267 0.5615234  0.4503824  0.44767267 0.69158584\n",
      " 0.5615234  0.5615234  0.4503824  0.5615234  0.4503824  0.69158584\n",
      " 0.4223884  0.69158584 0.44767267 0.4223884  0.5615234  0.69158584\n",
      " 0.4503824  0.3602188  0.4223884  0.44767267 0.3602188  0.69158584\n",
      " 0.69158584 0.49634358 0.4223884  0.3602188  0.5615234  0.4503824\n",
      " 0.4223884  0.4503824  0.5615234  0.4503824  0.3602188  0.44767267\n",
      " 0.44767267 0.33493453 0.4503824  0.69158584 0.49634358 0.4223884\n",
      " 0.44767267 0.4223884  0.44767267 0.69158584 0.44767267 0.49634358\n",
      " 0.4223884  0.4223884  0.6074846  0.33493453 0.44767267 0.4223884\n",
      " 0.4223884  0.69158584 0.3602188  0.44767267 0.44767267 0.5615234\n",
      " 0.4503824  0.4223884  0.5615234  0.3602188  0.5615234  0.4223884\n",
      " 0.33493453 0.44767267 0.5615234  0.4503824  0.4223884  0.4223884\n",
      " 0.3602188  0.69158584 0.4223884  0.3602188  0.5615234  0.3602188\n",
      " 0.44767267 0.44767267]\n"
     ]
    }
   ],
   "source": [
    "print(preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88462734 0.33787948 0.576506   0.88462734 0.88462734 0.576506\n",
      " 0.88462734 0.33787948 0.8140439  0.09674081 0.33787948 0.33787948\n",
      " 0.01093802 0.576506   0.33787948 0.33787948 0.33787948 0.33787948\n",
      " 0.33787948 0.09674081 0.01093802 0.576506   0.8140439  0.01093802\n",
      " 0.576506   0.576506   0.8140439  0.01093802 0.03767467 0.33787948\n",
      " 0.88462734 0.09674081 0.8140439  0.09674081 0.09674081 0.8140439\n",
      " 0.576506   0.8140439  0.576506   0.09674081 0.576506   0.8140439\n",
      " 0.88462734 0.88462734 0.09674081 0.01093802 0.33787948 0.09674081\n",
      " 0.33787948 0.576506   0.8140439  0.88462734 0.88462734 0.8140439\n",
      " 0.576506   0.09674081 0.576506   0.01093802 0.88462734 0.03767467\n",
      " 0.33787948 0.03767467 0.33787948 0.09674081 0.576506   0.01093802\n",
      " 0.576506   0.33787948 0.09674081 0.576506   0.33787948 0.09674081\n",
      " 0.33787948 0.576506   0.576506   0.576506   0.88462734 0.576506\n",
      " 0.33787948 0.33787948 0.09674081 0.576506   0.8140439  0.88462734\n",
      " 0.33787948 0.576506   0.03767467 0.01093802 0.09674081 0.8140439\n",
      " 0.33787948 0.33787948 0.88462734 0.33787948 0.33787948 0.8140439\n",
      " 0.09674081 0.576506   0.8140439  0.09674081 0.01093802 0.33787948\n",
      " 0.09674081 0.576506   0.33787948 0.88462734 0.09674081 0.576506\n",
      " 0.88462734 0.576506   0.88462734 0.09674081 0.576506   0.576506\n",
      " 0.88462734 0.09674081 0.576506   0.8140439  0.33787948 0.09674081\n",
      " 0.8140439  0.88462734 0.01093802 0.8140439  0.33787948 0.576506\n",
      " 0.8140439  0.33787948 0.8140439  0.33787948 0.8140439  0.33787948\n",
      " 0.33787948 0.33787948 0.576506   0.8140439  0.576506   0.88462734\n",
      " 0.09674081 0.8140439  0.8140439  0.88462734 0.01093802 0.09674081\n",
      " 0.09674081 0.576506   0.09674081 0.09674081 0.8140439  0.8140439\n",
      " 0.576506   0.8140439  0.8140439  0.576506   0.576506   0.33787948\n",
      " 0.09674081 0.88462734 0.88462734 0.09674081 0.576506   0.09674081\n",
      " 0.8140439  0.01093802 0.33787948 0.09674081 0.88462734 0.09674081\n",
      " 0.01093802 0.576506   0.88462734 0.33787948 0.576506   0.576506\n",
      " 0.33787948 0.8140439  0.8140439  0.8140439  0.8140439  0.09674081\n",
      " 0.8140439  0.8140439  0.88462734 0.88462734 0.576506   0.88462734\n",
      " 0.576506   0.576506   0.33787948 0.33787948 0.88462734 0.33787948\n",
      " 0.09674081 0.8140439  0.33787948 0.576506   0.03767467 0.33787948\n",
      " 0.576506   0.88462734 0.01093802 0.88462734 0.09674081 0.88462734\n",
      " 0.8140439  0.576506   0.33787948 0.88462734 0.576506   0.33787948\n",
      " 0.8140439  0.33787948 0.576506   0.576506   0.88462734 0.88462734\n",
      " 0.8140439  0.33787948 0.88462734 0.33787948 0.88462734 0.8140439\n",
      " 0.33787948 0.01093802 0.09674081 0.576506   0.8140439  0.8140439\n",
      " 0.33787948 0.576506   0.576506   0.33787948 0.8140439  0.8140439\n",
      " 0.88462734 0.09674081 0.09674081 0.576506   0.33787948 0.88462734\n",
      " 0.33787948 0.09674081 0.576506   0.576506   0.576506   0.8140439\n",
      " 0.576506   0.09674081 0.88462734 0.09674081 0.33787948 0.8140439\n",
      " 0.8140439  0.8140439  0.33787948 0.03767467 0.09674081 0.8140439\n",
      " 0.33787948 0.01093802 0.8140439  0.09674081 0.576506   0.03767467\n",
      " 0.33787948 0.576506   0.09674081 0.09674081 0.8140439  0.576506\n",
      " 0.33787948 0.33787948 0.33787948 0.8140439  0.8140439  0.09674081\n",
      " 0.576506   0.33787948 0.88462734 0.09674081 0.01093802 0.33787948\n",
      " 0.576506   0.33787948 0.33787948 0.33787948 0.576506   0.09674081\n",
      " 0.88462734 0.09674081 0.33787948 0.09674081 0.576506   0.576506\n",
      " 0.8140439  0.576506   0.03767467 0.01093802 0.576506   0.33787948\n",
      " 0.01093802 0.09674081 0.576506   0.33787948 0.33787948 0.33787948\n",
      " 0.88462734 0.576506   0.33787948 0.33787948 0.576506   0.88462734\n",
      " 0.33787948 0.576506   0.8140439  0.01093802 0.8140439  0.33787948\n",
      " 0.88462734 0.576506   0.33787948 0.576506   0.33787948 0.576506\n",
      " 0.09674081 0.88462734 0.88462734 0.88462734 0.33787948 0.33787948\n",
      " 0.576506   0.8140439  0.8140439  0.33787948 0.09674081 0.01093802\n",
      " 0.01093802 0.8140439  0.576506   0.33787948 0.8140439  0.576506\n",
      " 0.33787948 0.88462734 0.576506   0.33787948 0.8140439  0.8140439\n",
      " 0.33787948 0.33787948 0.33787948 0.576506   0.33787948 0.576506\n",
      " 0.8140439  0.576506   0.88462734 0.8140439  0.09674081 0.88462734\n",
      " 0.09674081 0.01093802 0.576506   0.8140439  0.03767467 0.576506\n",
      " 0.576506   0.8140439  0.576506   0.8140439  0.33787948 0.88462734\n",
      " 0.09674081 0.09674081 0.8140439  0.88462734 0.09674081 0.8140439\n",
      " 0.33787948 0.576506   0.09674081 0.33787948 0.576506   0.576506\n",
      " 0.09674081 0.576506   0.09674081 0.88462734 0.88462734 0.8140439\n",
      " 0.576506   0.576506   0.88462734 0.09674081 0.01093802 0.09674081\n",
      " 0.33787948 0.33787948 0.576506   0.576506   0.01093802 0.88462734\n",
      " 0.8140439  0.8140439  0.576506   0.576506   0.8140439  0.88462734\n",
      " 0.09674081 0.8140439  0.03767467 0.33787948 0.576506   0.88462734\n",
      " 0.576506   0.01093802 0.576506   0.33787948 0.01093802 0.8140439\n",
      " 0.88462734 0.576506   0.8140439  0.33787948 0.8140439  0.576506\n",
      " 0.576506   0.88462734 0.576506   0.576506   0.09674081 0.01093802\n",
      " 0.01093802 0.09674081 0.88462734 0.88462734 0.33787948 0.33787948\n",
      " 0.01093802 0.33787948 0.09674081 0.88462734 0.33787948 0.576506\n",
      " 0.33787948 0.576506   0.03767467 0.576506   0.33787948 0.576506\n",
      " 0.576506   0.88462734 0.01093802 0.576506   0.33787948 0.8140439\n",
      " 0.33787948 0.576506   0.03767467 0.01093802 0.8140439  0.576506\n",
      " 0.09674081 0.33787948 0.576506   0.576506   0.576506   0.33787948\n",
      " 0.33787948 0.576506   0.33787948 0.03767467 0.8140439  0.09674081\n",
      " 0.33787948 0.33787948] [0.5615234  0.4223884  0.4503824  0.69158584 0.5615234  0.5615234\n",
      " 0.69158584 0.44767267 0.5615234  0.3602188  0.4223884  0.3602188\n",
      " 0.3602188  0.49634358 0.4223884  0.49634358 0.44767267 0.4223884\n",
      " 0.3602188  0.44767267 0.3602188  0.4223884  0.5615234  0.3602188\n",
      " 0.6074846  0.4503824  0.69158584 0.3602188  0.44767267 0.33493453\n",
      " 0.69158584 0.3602188  0.69158584 0.44767267 0.4223884  0.69158584\n",
      " 0.4503824  0.5615234  0.4503824  0.4223884  0.69158584 0.5615234\n",
      " 0.69158584 0.5615234  0.44767267 0.3602188  0.4503824  0.4223884\n",
      " 0.49634358 0.4503824  0.33493453 0.69158584 0.69158584 0.5615234\n",
      " 0.5615234  0.44767267 0.4223884  0.3602188  0.69158584 0.44767267\n",
      " 0.49634358 0.3602188  0.4223884  0.3602188  0.5615234  0.3602188\n",
      " 0.5615234  0.33493453 0.44767267 0.33493453 0.3602188  0.44767267\n",
      " 0.44767267 0.5615234  0.5615234  0.4503824  0.69158584 0.4223884\n",
      " 0.4223884  0.33493453 0.44767267 0.5615234  0.69158584 0.69158584\n",
      " 0.6074846  0.4223884  0.44767267 0.3602188  0.4223884  0.4503824\n",
      " 0.44767267 0.3602188  0.69158584 0.4223884  0.4223884  0.4223884\n",
      " 0.44767267 0.4223884  0.69158584 0.3602188  0.3602188  0.4223884\n",
      " 0.44767267 0.4503824  0.5615234  0.69158584 0.3602188  0.5615234\n",
      " 0.69158584 0.4223884  0.5615234  0.44767267 0.4503824  0.4503824\n",
      " 0.69158584 0.4223884  0.4223884  0.69158584 0.4223884  0.44767267\n",
      " 0.4223884  0.69158584 0.3602188  0.69158584 0.49634358 0.5615234\n",
      " 0.33493453 0.4223884  0.5615234  0.4503824  0.5615234  0.44767267\n",
      " 0.49634358 0.4223884  0.5615234  0.5615234  0.4223884  0.69158584\n",
      " 0.44767267 0.5615234  0.5615234  0.69158584 0.44767267 0.4223884\n",
      " 0.3602188  0.4223884  0.44767267 0.3602188  0.69158584 0.5615234\n",
      " 0.5615234  0.5615234  0.5615234  0.5615234  0.5615234  0.44767267\n",
      " 0.44767267 0.69158584 0.69158584 0.3602188  0.5615234  0.3602188\n",
      " 0.4223884  0.3602188  0.4223884  0.44767267 0.5615234  0.3602188\n",
      " 0.3602188  0.4223884  0.69158584 0.4223884  0.44767267 0.5615234\n",
      " 0.4503824  0.5615234  0.5615234  0.5615234  0.69158584 0.3602188\n",
      " 0.69158584 0.5615234  0.69158584 0.69158584 0.33493453 0.69158584\n",
      " 0.5615234  0.4223884  0.4223884  0.33493453 0.69158584 0.3602188\n",
      " 0.4223884  0.69158584 0.4503824  0.4223884  0.3602188  0.4503824\n",
      " 0.4503824  0.69158584 0.44767267 0.5615234  0.3602188  0.69158584\n",
      " 0.5615234  0.4223884  0.44767267 0.5615234  0.5615234  0.4223884\n",
      " 0.5615234  0.4223884  0.4223884  0.5615234  0.69158584 0.4503824\n",
      " 0.4223884  0.44767267 0.69158584 0.3602188  0.69158584 0.4503824\n",
      " 0.44767267 0.3602188  0.44767267 0.5615234  0.5615234  0.4503824\n",
      " 0.49634358 0.4223884  0.4503824  0.4223884  0.5615234  0.5615234\n",
      " 0.69158584 0.3602188  0.44767267 0.4223884  0.33493453 0.69158584\n",
      " 0.4223884  0.44767267 0.49634358 0.4503824  0.5615234  0.5615234\n",
      " 0.4503824  0.3602188  0.69158584 0.3602188  0.33493453 0.5615234\n",
      " 0.4503824  0.33493453 0.44767267 0.44767267 0.3602188  0.5615234\n",
      " 0.3602188  0.44767267 0.5615234  0.44767267 0.69158584 0.44767267\n",
      " 0.69158584 0.4503824  0.4223884  0.44767267 0.5615234  0.4223884\n",
      " 0.3602188  0.3602188  0.3602188  0.5615234  0.69158584 0.3602188\n",
      " 0.4503824  0.4223884  0.69158584 0.3602188  0.49634358 0.4503824\n",
      " 0.5615234  0.3602188  0.4223884  0.44767267 0.33493453 0.44767267\n",
      " 0.5615234  0.49634358 0.4223884  0.3602188  0.4223884  0.5615234\n",
      " 0.4503824  0.5615234  0.3602188  0.44767267 0.5615234  0.44767267\n",
      " 0.44767267 0.3602188  0.4223884  0.3602188  0.44767267 0.33493453\n",
      " 0.69158584 0.5615234  0.4223884  0.49634358 0.5615234  0.5615234\n",
      " 0.3602188  0.5615234  0.5615234  0.44767267 0.49634358 0.44767267\n",
      " 0.5615234  0.69158584 0.3602188  0.69158584 0.44767267 0.4503824\n",
      " 0.44767267 0.69158584 0.69158584 0.69158584 0.4223884  0.4223884\n",
      " 0.5615234  0.5615234  0.5615234  0.4503824  0.44767267 0.3602188\n",
      " 0.3602188  0.5615234  0.4223884  0.4223884  0.69158584 0.4223884\n",
      " 0.44767267 0.69158584 0.5615234  0.4223884  0.4223884  0.5615234\n",
      " 0.3602188  0.44767267 0.69158584 0.4503824  0.3602188  0.33493453\n",
      " 0.5615234  0.4503824  0.69158584 0.5615234  0.33493453 0.69158584\n",
      " 0.3602188  0.3602188  0.4223884  0.4503824  0.3602188  0.4223884\n",
      " 0.4503824  0.5615234  0.4223884  0.69158584 0.4223884  0.69158584\n",
      " 0.3602188  0.33493453 0.5615234  0.5615234  0.44767267 0.5615234\n",
      " 0.3602188  0.4503824  0.44767267 0.5615234  0.4503824  0.49634358\n",
      " 0.3602188  0.4503824  0.3602188  0.69158584 0.69158584 0.5615234\n",
      " 0.4503824  0.49634358 0.69158584 0.49634358 0.3602188  0.3602188\n",
      " 0.4223884  0.44767267 0.5615234  0.4503824  0.44767267 0.69158584\n",
      " 0.5615234  0.5615234  0.4503824  0.5615234  0.4503824  0.69158584\n",
      " 0.4223884  0.69158584 0.44767267 0.4223884  0.5615234  0.69158584\n",
      " 0.4503824  0.3602188  0.4223884  0.44767267 0.3602188  0.69158584\n",
      " 0.69158584 0.49634358 0.4223884  0.3602188  0.5615234  0.4503824\n",
      " 0.4223884  0.4503824  0.5615234  0.4503824  0.3602188  0.44767267\n",
      " 0.44767267 0.33493453 0.4503824  0.69158584 0.49634358 0.4223884\n",
      " 0.44767267 0.4223884  0.44767267 0.69158584 0.44767267 0.49634358\n",
      " 0.4223884  0.4223884  0.6074846  0.33493453 0.44767267 0.4223884\n",
      " 0.4223884  0.69158584 0.3602188  0.44767267 0.44767267 0.5615234\n",
      " 0.4503824  0.4223884  0.5615234  0.3602188  0.5615234  0.4223884\n",
      " 0.33493453 0.44767267 0.5615234  0.4503824  0.4223884  0.4223884\n",
      " 0.3602188  0.69158584 0.4223884  0.3602188  0.5615234  0.3602188\n",
      " 0.44767267 0.44767267]\n"
     ]
    }
   ],
   "source": [
    "print(preds_train, preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_array = np.zeros(preds_train.shape[0]*2)\n",
    "i = 0\n",
    "j = 0\n",
    "while True:\n",
    "    \n",
    "    if i == preds_train.shape[0]:\n",
    "        break\n",
    "    \n",
    "    res_array[j] = preds_val[i]\n",
    "    j += 1\n",
    "    res_array[j] = preds_train[i]\n",
    "    j += 1\n",
    "    \n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(\"sample_submission.csv\", index_col = 0)\n",
    "submit.Predicted = res_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
