{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "nn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBwYByG1nGZo",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "65046308-809c-4b1c-c1f3-d1e9cd9f0ad0"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-35f637e8-227e-4038-984e-9b90f52cb2c0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-35f637e8-227e-4038-984e-9b90f52cb2c0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_submission.csv to sample_submission.csv\n",
            "Saving submission.csv to submission.csv\n",
            "Saving test.csv to test.csv\n",
            "Saving train.csv to train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW8xklkFnGZv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "87ee29a7-6f1a-4bef-f04b-18bd88ec185e"
      },
      "source": [
        "import io\n",
        "print(\"I am here!\")\n",
        "\n",
        "my_data = pd.read_csv(\"train.csv\", index_col = 0)\n",
        "\n",
        "my_data = my_data.drop(columns = ['batch_size_test', 'batch_size_val', 'criterion', 'optimizer', 'batch_size_train'])\n",
        "y_train = my_data['train_error']\n",
        "y_val = my_data['val_error']\n",
        "my_data = my_data.drop(columns = 'train_error')\n",
        "my_data = my_data.drop(columns = 'val_error')\n",
        "\n",
        "x = my_data.drop(columns = ['train_loss','val_loss','arch_and_hp','init_params_mu', 'init_params_std', 'init_params_l2'])\n",
        "\n",
        "col_train = [col for col in x.filter(regex='^train',axis=1).columns]\n",
        "col_val = [col for col in x.filter(regex='^val',axis=1).columns]\n",
        "#print(col_train)\n",
        "#print(col_val)\n",
        "#print(x)\n",
        "'''\n",
        "\n",
        "# ----------------- this part we split the train set to improve the performence of the model ------------\n",
        "\n",
        "#1. try on training dataset\n",
        "A = x.drop(['id'],axis=1)\n",
        "train = A[col_train]\n",
        "#val = x[col_val]\n",
        "val = A[col_val]\n",
        "\n",
        "#2. split 70%/30%\n",
        "\n",
        "# ==== 70% as the train data ==== y means target ====\n",
        "\n",
        "split_train_train = train[:int(len(train)*0.7)] #/\n",
        "split_train_val = val[:int(len(train)*0.7)]\n",
        "\n",
        "split_y_train_train = y_train[:int(len(train)*0.7)] #/\n",
        "split_y_train_val = y_val[:int(len(train)*0.7)]\n",
        "\n",
        "\n",
        "# ==== 30% as the train data ==== y means target ====\n",
        "\n",
        "\n",
        "split_test_train = train[int(len(train)*0.7):] #/\n",
        "split_test_val = val[int(len(train)*0.7):]\n",
        "\n",
        "split_y_test_train = y_train[int(len(train)*0.7):]\n",
        "split_y_test_val = y_val[int(len(train)*0.7):]\n",
        "\n",
        "#print(split_train_train)\n",
        "\n",
        "# === train part (70%) ===\n",
        "\n",
        "torch_tensor_train = torch.tensor([split_train_train[title].values for title in col_train],dtype=torch.float)\n",
        "torch_tensor_val = torch.tensor([split_train_val[title].values for title in col_val],dtype=torch.float)\n",
        "\n",
        "torch_tensor_train = torch.t(torch_tensor_train)\n",
        "torch_tensor_val = torch.t(torch_tensor_val)\n",
        "\n",
        "# === test part (30%) ===\n",
        "\n",
        "torch_tensor_test_train = torch.tensor([split_test_train[title].values for title in col_train],dtype=torch.float)\n",
        "torch_tensor_test_val = torch.tensor([split_test_val[title].values for title in col_val],dtype=torch.float)\n",
        "\n",
        "torch_tensor_test_train = torch.t(torch_tensor_test_train)\n",
        "torch_tensor_test_val = torch.t(torch_tensor_test_val)\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am here!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n# ----------------- this part we split the train set to improve the performence of the model ------------\\n\\n#1. try on training dataset\\nA = x.drop(['id'],axis=1)\\ntrain = A[col_train]\\n#val = x[col_val]\\nval = A[col_val]\\n\\n#2. split 70%/30%\\n\\n# ==== 70% as the train data ==== y means target ====\\n\\nsplit_train_train = train[:int(len(train)*0.7)] #/\\nsplit_train_val = val[:int(len(train)*0.7)]\\n\\nsplit_y_train_train = y_train[:int(len(train)*0.7)] #/\\nsplit_y_train_val = y_val[:int(len(train)*0.7)]\\n\\n\\n# ==== 30% as the train data ==== y means target ====\\n\\n\\nsplit_test_train = train[int(len(train)*0.7):] #/\\nsplit_test_val = val[int(len(train)*0.7):]\\n\\nsplit_y_test_train = y_train[int(len(train)*0.7):]\\nsplit_y_test_val = y_val[int(len(train)*0.7):]\\n\\n#print(split_train_train)\\n\\n# === train part (70%) ===\\n\\ntorch_tensor_train = torch.tensor([split_train_train[title].values for title in col_train],dtype=torch.float)\\ntorch_tensor_val = torch.tensor([split_train_val[title].values for title in col_val],dtype=torch.float)\\n\\ntorch_tensor_train = torch.t(torch_tensor_train)\\ntorch_tensor_val = torch.t(torch_tensor_val)\\n\\n# === test part (30%) ===\\n\\ntorch_tensor_test_train = torch.tensor([split_test_train[title].values for title in col_train],dtype=torch.float)\\ntorch_tensor_test_val = torch.tensor([split_test_val[title].values for title in col_val],dtype=torch.float)\\n\\ntorch_tensor_test_train = torch.t(torch_tensor_test_train)\\ntorch_tensor_test_val = torch.t(torch_tensor_test_val)\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlXpFv2BnGZz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e2ce1a36-0640-4d8c-be35-3fff5614f222"
      },
      "source": [
        "# -------------- the following represents the whole test data set ------------------------\n",
        "\n",
        "torch_tensor_train = torch.tensor([x[title].values for title in col_train],dtype=torch.float)\n",
        "torch_tensor_val = torch.tensor([x[title].values for title in col_val],dtype=torch.float)\n",
        "\n",
        "\n",
        "torch_tensor_train = torch.t(torch_tensor_train)\n",
        "torch_tensor_val = torch.t(torch_tensor_val)\n",
        "\n",
        "\n",
        "print(len(col_train))\n",
        "print(torch_tensor_train.size())\n",
        "\n",
        "\n",
        "# ------------- test part -----------------------\n",
        "\n",
        "test_data = pd.read_csv(\"test.csv\", index_col = 0)\n",
        "test_data = test_data.drop(columns = ['batch_size_test', 'batch_size_val', 'criterion', 'optimizer', 'batch_size_train'])\n",
        "test_data = test_data.drop(columns = ['arch_and_hp','init_params_mu', 'init_params_std', 'init_params_l2'])\n",
        "test_train = test_data[col_train]\n",
        "test_val = test_data[col_val]\n",
        "\n",
        "torch_tensor_test_train = torch.tensor([test_train[title].values for title in col_train],dtype=torch.float)\n",
        "torch_tensor_test_val = torch.tensor([test_val[title].values for title in col_val],dtype=torch.float)\n",
        "\n",
        "torch_tensor_test_train = torch.t(torch_tensor_test_train)\n",
        "torch_tensor_test_val = torch.t(torch_tensor_test_val)\n",
        "\n",
        "#print(test_train)\n",
        "print(torch_tensor_test_train.size())\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "torch.Size([1878, 100])\n",
            "torch.Size([476, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhKDhNUBnGZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a73dcfcd-7580-4be8-934f-508ce1d05031"
      },
      "source": [
        "#we first select a naive model after sorting based on the lowest val_error\n",
        "\n",
        "#model = torch.nn.Sequential(nn.BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), nn.Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1)) , nn.Tanh() , nn.Dropout2d(p=0.4569261378573387, inplace=False) , nn.Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1)) , nn.ReLU() , nn.Dropout2d(p=0.022897457910152297, inplace=False) , nn.Conv2d(27, 28, kernel_size=(5, 5), stride=(1, 1)) , nn.LeakyReLU(negative_slope=0.5719652455913541) , nn.MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False) , nn.Flatten() , nn.Linear(in_features=700, out_features=28, bias=True) , nn.LeakyReLU(negative_slope=0.48187737007312437) , nn.Dropout(p=0.3735548245474202, inplace=False) , nn.Linear(in_features=28, out_features=10, bias=True) , nn.BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) )\n",
        "\n",
        "\n",
        "input_size = 100\n",
        "hidden_sizes = [60, 30]\n",
        "output_size = 1\n",
        "\n",
        "\n",
        "model_train = nn.Sequential(\n",
        "                      nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      )\n",
        "#nn.Softmax(dim=0)\n",
        "# Define the loss\n",
        "criterion = nn.MSELoss()\n",
        "# Optimizers require the parameters to optimize and a learning rate\n",
        "optimizer = torch.optim.SGD(model_train.parameters(), lr=0.003)\n",
        "epochs = 1000\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for i in range(torch_tensor_train.size()[0]):\n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model_train(torch_tensor_train[i])\n",
        "        #print(output)\n",
        "        #torch.tensor(y_train[i])\n",
        "        loss = criterion(output, torch.tensor(y_train[i]))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "\n",
        "        \n",
        "model_val = nn.Sequential(\n",
        "                      nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      )\n",
        "\n",
        "# Optimizers require the parameters to optimize and a learning rate\n",
        "optimizer = torch.optim.SGD(model_val.parameters(), lr=0.008)\n",
        "epochs = 1000\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for i in range(torch_tensor_train.size()[0]):\n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model_val(torch_tensor_val[i])\n",
        "        #print(output)\n",
        "        loss = criterion(output, torch.tensor(y_val[i]))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "\n",
        "print(\"finished\")\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z05QvXJVnGZ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e4a524d-3d9c-4bac-a312-4f326ac59e96"
      },
      "source": [
        "\n",
        "#now we do the prediction\n",
        "\n",
        "print(\"I am here!\")\n",
        "res_train,res_val = [],[]\n",
        "\n",
        "\n",
        "for row in torch_tensor_test_train:\n",
        "    res_train.append(model_train(row).item())\n",
        "\n",
        "for row in torch_tensor_test_val:\n",
        "    res_val.append(model_val(row).item())\n",
        "\n",
        "#print(len(res_val))\n",
        "#print(res_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am here!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7TaOQXBnGZ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6f4be117-9c4f-4624-f978-74c29d96e66f"
      },
      "source": [
        "'''\n",
        "\n",
        "preds_train = np.array(res_train)\n",
        "preds_val = np.array(res_val)\n",
        "\n",
        "\n",
        "R_square = 1 - (sum((preds_train - split_y_test_train)**2)/sum((preds_train - preds_train.mean())**2))\n",
        "print(R_square)\n",
        "\n",
        "R_square = 1 - (sum((preds_val - split_y_test_val)**2)/sum((preds_val - preds_val.mean())**2))\n",
        "print(R_square)\n",
        "\n",
        "'''"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9692663366213935\n",
            "0.8349412415070974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvToUuyMnGaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5d88062-9d04-411d-ef02-69e3f851ce30"
      },
      "source": [
        "\n",
        "submit = pd.read_csv(\"sample_submission.csv\", index_col = 0)\n",
        "res_array = np.zeros(len(res_train)*2)\n",
        "\n",
        "\n",
        "i = 0\n",
        "j = 0\n",
        "while True:\n",
        "    \n",
        "    if i == len(res_train):\n",
        "        break\n",
        "    \n",
        "    res_array[j] = res_val[i]\n",
        "    j += 1\n",
        "    res_array[j] = res_train[i]\n",
        "    j += 1\n",
        "    \n",
        "    i += 1 \n",
        "\n",
        "print(res_array)\n",
        "\n",
        "\n",
        "\n",
        "submit.Predicted = res_array\n",
        "submit.to_csv(\"submission.csv\")\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.54883868  0.85082996  0.39386749  0.37556946  0.52822775  0.61002874\n",
            "  0.67002648  0.84686571  0.58538717  0.83720905  0.55757582  0.53027081\n",
            "  0.73469955  0.88511312  0.4242301   0.38845819  0.67616898  0.80692828\n",
            "  0.31501901  0.21129754  0.46801496  0.42407858  0.3870149   0.38994023\n",
            "  0.36243737 -0.02285412  0.49161145  0.63368309  0.41775081  0.22485672\n",
            "  0.51769352  0.47489434  0.44534716  0.2298882   0.42675769  0.45466068\n",
            "  0.39131746  0.17932351  0.44834331  0.07845327  0.38032994  0.01696175\n",
            "  0.41024628  0.51740026  0.60148591  0.77381158  0.29269931  0.01002058\n",
            "  0.5665884   0.64899039  0.46934178  0.53119862  0.67269188  0.82407022\n",
            "  0.37272978  0.0422889   0.43653747  0.04758126  0.35215116  0.30813476\n",
            "  0.76950014  0.87740248  0.37408939  0.22225241  0.70097923  0.79272985\n",
            "  0.47123185  0.06158581  0.48159188  0.04142225  0.62132889  0.73621362\n",
            "  0.51696885  0.5578897   0.60754561  0.80464244  0.45649755  0.58189762\n",
            "  0.45888162  0.1497747   0.65624505  0.60727739  0.63755238  0.81929201\n",
            "  0.71738315  0.87401414  0.61762202  0.87066114  0.43963298  0.16628224\n",
            "  0.36472419  0.04110092  0.52486306  0.24583356  0.43573582  0.25997335\n",
            "  0.51846147  0.48459905  0.49267513  0.58232003  0.34240705  0.70136338\n",
            "  0.6538347   0.86543608  0.71730173  0.87534809  0.54107153  0.72967017\n",
            "  0.53820556  0.58625615  0.42420441  0.04457068  0.43957415  0.54045099\n",
            "  0.37201554  0.00758344  0.67976934  0.86610472  0.45348316  0.02383938\n",
            "  0.4783878   0.43719876  0.36411035  0.01468381  0.43177837  0.40981495\n",
            "  0.38857001  0.06840977  0.52329689  0.66825306  0.36303365  0.01708049\n",
            "  0.5805366   0.69849658  0.36197096  0.31066713  0.44786721  0.14827152\n",
            "  0.3713485   0.5367884   0.31469491  0.26835132  0.45338255  0.21720822\n",
            "  0.4456768   0.36558402  0.46449122  0.58887672  0.5248493   0.67551064\n",
            "  0.52351701  0.68827057  0.65938687  0.84319091  0.3893472   0.5899294\n",
            "  0.3910464   0.3063969   0.37974319  0.34810686  0.43665662  0.23140627\n",
            "  0.54724902  0.71285397  0.62537193  0.7136026   0.72037387  0.87719715\n",
            "  0.56268179  0.1115211   0.39569691  0.46065223  0.43535638  0.00794756\n",
            "  0.3769685   0.01259431  0.41143489  0.19404906  0.44415772  0.69427907\n",
            "  0.43094236  0.17282632  0.34702387  0.42520511  0.68613493  0.84160322\n",
            "  0.39409512  0.32952207  0.49021378  0.2682777   0.39341369  0.82371116\n",
            "  0.44746324  0.23830509  0.438979    0.49973443  0.6684407   0.81533873\n",
            "  0.31327403  0.08133665  0.34572387  0.0054186   0.42373198  0.36297718\n",
            "  0.47332329  0.2048534   0.50929326  0.62370229  0.57605582  0.22495991\n",
            "  0.71693313  0.87715393  0.37725958  0.05571201  0.62757331  0.48433656\n",
            "  0.67288524  0.86640054  0.41276857  0.45894313  0.52483076  0.86198366\n",
            "  0.45306602  0.0596348   0.49354044  0.52962399  0.47521785  0.56857061\n",
            "  0.72303843  0.87621886  0.46097597  0.09989992  0.4312588   0.56471425\n",
            "  0.67769128  0.79100347  0.4341549   0.44094408  0.46806777  0.05891874\n",
            "  0.42544293  0.72621524  0.71802741  0.87574279  0.32594818  0.01593119\n",
            "  0.65564853  0.7918216   0.46008113  0.41332316  0.54414773  0.63998473\n",
            "  0.32607278  0.82508826  0.41021949  0.38640827  0.58322942  0.75083005\n",
            "  0.46723834  0.32813567  0.53052157  0.70564747  0.45244515  0.31485045\n",
            "  0.47405237  0.26610827  0.43578953  0.30733141  0.55859625  0.65594822\n",
            "  0.62651837  0.73618627  0.41369349  0.46294433  0.72149509  0.87686014\n",
            "  0.45924291  0.23148954  0.64973235  0.82936776  0.58897769  0.82724619\n",
            "  0.6629492   0.87161314  0.45174515  0.01525241  0.42626157  0.14237377\n",
            "  0.37077752  0.26262903  0.47228295  0.56071281  0.49652025  0.06979883\n",
            "  0.37201744  0.04755548  0.7192992   0.81929517  0.62596989  0.77281642\n",
            "  0.51837885  0.64019948  0.60885268  0.83020395  0.58409232  0.76471341\n",
            "  0.54265159  0.59683007  0.5745284   0.56895244  0.4625482   0.15275241\n",
            "  0.45921144  0.15700285  0.72172004  0.87828958  0.71022123  0.8578335\n",
            "  0.37005538  0.11286108  0.5357281   0.4968594   0.40526098  0.05517194\n",
            "  0.3569423   0.82445222  0.32521409  0.01914886  0.39852983  0.37749991\n",
            "  0.47301114  0.1720251   0.52258134  0.86849713  0.42976552  0.05679449\n",
            "  0.3467882   0.00843775  0.42938817  0.47085324  0.78134465  0.86432105\n",
            "  0.39919841  0.24651034  0.43615088  0.45448864  0.48417795  0.55912083\n",
            "  0.4459632   0.4596591   0.51043928  0.7517001   0.54469138  0.68975484\n",
            "  0.50678527  0.71221828  0.65157849  0.83056343  0.34822208  0.11641426\n",
            "  0.68061161  0.78558326  0.63643396  0.7192809   0.64912587  0.88172746\n",
            "  0.71405339  0.87915641  0.3296124   0.46561417  0.71822655  0.87771934\n",
            "  0.6455968   0.51227379  0.40557563  0.6385681   0.41527274  0.35074878\n",
            "  0.38899767  0.37814653  0.70444399  0.87235481  0.31679362  0.37582111\n",
            "  0.45974824  0.09693372  0.6973865   0.82770383  0.51943266  0.33516654\n",
            "  0.44657457  0.5581187   0.38184145  0.02562386  0.53278762  0.47358045\n",
            "  0.4861784   0.52336884  0.69830853  0.8769688   0.47186539  0.00823456\n",
            "  0.57745433  0.88646877  0.28370953  0.20952563  0.66248059  0.87542665\n",
            "  0.5432893   0.80471152  0.40652424  0.58712274  0.47792837  0.22112811\n",
            "  0.5834291   0.88018107  0.65899122  0.56697518  0.44074872  0.28563932\n",
            "  0.54298335  0.76520747  0.43401778  0.43320012  0.45055321  0.5290435\n",
            "  0.53882492  0.49781033  0.63065231  0.84882981  0.50745595  0.85772234\n",
            "  0.43882096  0.68476313  0.41518497  0.38712412  0.71979529  0.88097906\n",
            "  0.41773656  0.2262136   0.72196627  0.87750137  0.49326804  0.71847135\n",
            "  0.46338657  0.28154519  0.35396966 -0.00664291  0.49102193  0.22395268\n",
            "  0.5165922   0.65221971  0.50690901  0.68322951  0.50727379  0.70377028\n",
            "  0.45912525  0.39205042  0.47517172  0.51040518  0.53072459  0.65974736\n",
            "  0.41852647  0.28477526  0.60850304  0.82043082  0.51554734  0.67797613\n",
            "  0.71152622  0.87799007  0.28472024  0.24471231  0.49341765  0.05589899\n",
            "  0.43847764  0.62672663  0.35827842  0.38975146  0.71926743  0.87702262\n",
            "  0.43527406  0.41876298  0.45408902  0.06208318  0.49997506  0.48436114\n",
            "  0.48303196  0.56863058  0.58100033  0.5906505   0.51226783  0.7586273\n",
            "  0.45936474  0.54374439  0.37278399  0.06764054  0.71903241  0.87445831\n",
            "  0.37752658  0.14004335  0.33482409  0.27381498  0.52032804  0.72299397\n",
            "  0.49700993  0.75271368  0.34385854  0.79210329  0.42844474  0.42286706\n",
            "  0.42678547  0.03886423  0.32452196  0.14947748  0.55100524  0.71566427\n",
            "  0.37329984  0.18101558  0.4479802   0.01217887  0.57529479  0.81073868\n",
            "  0.44233793  0.07960415  0.702461    0.50379419  0.46172291  0.04014057\n",
            "  0.75842863  0.40856272  0.45026845  0.56392616  0.40811631  0.22638142\n",
            "  0.46166903  0.16020972  0.61465943  0.82112002  0.42379171  0.4840959\n",
            "  0.36337888  0.33153898  0.42102641  0.14048982  0.38012019  0.30632946\n",
            "  0.60399127  0.80574423  0.70121634  0.67153525  0.36563358  0.06878963\n",
            "  0.51351047  0.62235272  0.42798847  0.23800935  0.71737427  0.87686443\n",
            "  0.36129665  0.06533647  0.49727154 -0.00120509  0.48345205  0.24129686\n",
            "  0.61476958  0.52966267  0.31232077  0.28497666  0.44011936  0.44327316\n",
            "  0.41202733  0.43603188  0.33292022  0.57172298  0.47219166  0.08578327\n",
            "  0.61325079  0.84412467  0.47400296  0.21443631  0.43776941  0.36772445\n",
            "  0.40651238  0.08553222  0.41112003  0.61444855  0.5670256   0.69739008\n",
            "  0.46791887  0.74321401  0.51337653  0.61383784  0.28851312  0.0425421\n",
            "  0.41527194  0.02718964  0.55123585  0.65558743  0.43538514  0.32205036\n",
            "  0.40076244  0.00799227  0.3788712   0.08768037  0.4504011   0.51546884\n",
            "  0.37815797  0.17000182  0.48373681  0.16282718  0.37033045  0.42584535\n",
            "  0.71789074  0.88583803  0.56951594  0.62967551  0.40651605  0.34580997\n",
            "  0.47403574  0.34002873  0.50418782  0.61591232  0.65962976  0.84347773\n",
            "  0.40583137  0.22702111  0.60364598  0.7533139   0.64361542  0.81187594\n",
            "  0.447624   -0.00573033  0.45485345  0.7579143   0.44201601  0.33975098\n",
            "  0.61300963  0.84556401  0.62747663  0.68000174  0.43259507  0.32813975\n",
            "  0.71997476  0.61076963  0.4577311   0.3058393   0.50330752  0.56208801\n",
            "  0.4821707   0.06060147  0.71896499  0.87566686  0.69784284  0.87048954\n",
            "  0.72437543  0.87787855  0.40154043  0.29298729  0.43160337  0.38668895\n",
            "  0.52751774  0.65733069  0.63852119  0.81210196  0.60809839  0.74465227\n",
            "  0.50393826  0.40323234  0.48972693  0.10343002  0.43563455  0.00619531\n",
            "  0.35602248  0.02160227  0.59629786  0.74250609  0.42533162  0.50163764\n",
            "  0.43487272  0.27772596  0.69222218  0.8010608   0.41030592  0.43287036\n",
            "  0.49515298  0.24905066  0.64870381  0.87476379  0.56239605  0.62938869\n",
            "  0.47248125  0.31995112  0.38323134  0.76869011  0.53280514  0.72239089\n",
            "  0.36827481  0.36359388  0.48041457  0.12885021  0.68128544  0.37092426\n",
            "  0.47677845  0.65232223  0.3639569   0.24436072  0.38728303  0.48362073\n",
            "  0.66754436  0.71997738  0.46836346  0.52047908  0.72143352  0.87631679\n",
            "  0.61472082  0.78402132  0.34917286  0.05841297  0.71628296  0.87658912\n",
            "  0.42773035  0.20941171  0.36078393  0.00892416  0.44417512  0.67889726\n",
            "  0.47850019  0.8271746   0.30888194  0.04737607  0.43721253  0.49731988\n",
            "  0.46250188  0.56813836  0.51786554  0.71423078  0.42500722  0.535133\n",
            "  0.69223171  0.77214146  0.46244431  0.41526353  0.71891677  0.8755219\n",
            "  0.39941615  0.03885716  0.3769902   0.14632212  0.55191678  0.82210535\n",
            "  0.60186458  0.86934578  0.46998855  0.16438232  0.54117388  0.80862349\n",
            "  0.36565188  0.36968002  0.47729284  0.62406433  0.48371184  0.2159023\n",
            "  0.51784134  0.37787724  0.47496071  0.58479792  0.4868415   0.40205005\n",
            "  0.42040303  0.09696263  0.42692512  0.59730566  0.38409662  0.14730468\n",
            "  0.89451033  0.88010323  0.72701836  0.87995338  0.52597249  0.70744431\n",
            "  0.51155251  0.60245299  0.47297487  0.48350161  0.71955186  0.87751651\n",
            "  0.51494288  0.01262328  0.38115844  0.00498071  0.30526534  0.23271564\n",
            "  0.4266679   0.42831555  0.48353815  0.21646868  0.66201699  0.59638131\n",
            "  0.4762086   0.5415203   0.44004446  0.0144797   0.70013112  0.8737967\n",
            "  0.60517317  0.78153008  0.57034051  0.79595745  0.4658792   0.6059041\n",
            "  0.51384884  0.46396405  0.45438793  0.68022668  0.70449108  0.85605848\n",
            "  0.41748878  0.05735159  0.6353265   0.7958554   0.44792026  0.02714348\n",
            "  0.42802852  0.45130312  0.49901333  0.41773495  0.71865994  0.88231719\n",
            "  0.46420419  0.66247284  0.3479152  -0.0246594   0.40215245  0.58361501\n",
            "  0.44606113  0.45182562  0.35385001  0.01253724  0.6549952   0.81211811\n",
            "  0.6288178   0.88129079  0.50811803  0.49877965  0.42317915  0.78241765\n",
            "  0.41546726  0.28015769  0.52057934  0.6854614   0.50340539  0.66885918\n",
            "  0.45043337  0.57946807  0.47946027  0.84079546  0.52100521  0.6536572\n",
            "  0.48098531  0.52270132  0.339531    0.03699404  0.44500792  0.00623295\n",
            "  0.42845267  0.02013701  0.38958442  0.2581768   0.42991376  0.87665802\n",
            "  0.71007943  0.8849895   0.54601419  0.24145968  0.41947529  0.40272599\n",
            "  0.34843141 -0.01051456  0.42817536  0.30691963  0.45523292  0.21415332\n",
            "  0.72238004  0.87566686  0.42541355  0.28739819  0.4673211   0.52605152\n",
            "  0.4358702   0.3658123   0.42862189  0.52094245  0.38354826  0.01777542\n",
            "  0.37796307  0.55025345  0.46417129  0.45148504  0.44310653  0.45612711\n",
            "  0.43037984  0.53980106  0.71753627  0.87593484  0.37502873  0.01672038\n",
            "  0.44066232  0.48043346  0.42940184  0.37871704  0.61297989  0.82801777\n",
            "  0.41958547  0.32311308  0.42737857  0.47926298  0.51958185  0.02810755\n",
            "  0.34490308 -0.00910768  0.54070973  0.71358263  0.40981248  0.51650852\n",
            "  0.34214532  0.21970129  0.44411787  0.29891765  0.51299638  0.6173563\n",
            "  0.48023307  0.4984253   0.44745457  0.62422431  0.44570151  0.4148131\n",
            "  0.395033    0.29252285  0.68345827  0.66601247  0.45992348  0.28528583\n",
            "  0.3695789   0.03822446  0.5312404   0.71678507  0.3924838   0.04587179\n",
            "  0.45807633  0.35682571  0.4721615   0.44476175]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5vlOtEknGaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}