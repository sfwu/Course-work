{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we first select a naive model after sorting based on the lowest val_error\n",
    "\n",
    "#model = torch.nn.Sequential(nn.BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), nn.Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1)) , nn.Tanh() , nn.Dropout2d(p=0.4569261378573387, inplace=False) , nn.Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1)) , nn.ReLU() , nn.Dropout2d(p=0.022897457910152297, inplace=False) , nn.Conv2d(27, 28, kernel_size=(5, 5), stride=(1, 1)) , nn.LeakyReLU(negative_slope=0.5719652455913541) , nn.MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False) , nn.Flatten() , nn.Linear(in_features=700, out_features=28, bias=True) , nn.LeakyReLU(negative_slope=0.48187737007312437) , nn.Dropout(p=0.3735548245474202, inplace=False) , nn.Linear(in_features=28, out_features=10, bias=True) , nn.BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) )\n",
    "\n",
    "input_size = 1000\n",
    "hidden_sizes = [500, 300]\n",
    "output_size = \n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n",
    "\n",
    "\n",
    "model = nn.Sequential(nn.Flatten(),\n",
    "                      nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv(\"train.csv\", index_col = 0)\n",
    "\n",
    "my_data = my_data.drop(columns = ['batch_size_test', 'batch_size_val', 'criterion', 'optimizer', 'batch_size_train'])\n",
    "y_train = my_data['train_error']\n",
    "y_val = my_data['val_error']\n",
    "my_data = my_data.drop(columns = 'train_error')\n",
    "my_data = my_data.drop(columns = 'val_error')\n",
    "\n",
    "x = my_data.drop(columns = ['train_loss','val_loss','arch_and_hp','init_params_mu', 'init_params_std', 'init_params_l2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_accs_0', 'train_accs_1', 'train_accs_2', 'train_accs_3', 'train_accs_4', 'train_accs_5', 'train_accs_6', 'train_accs_7', 'train_accs_8', 'train_accs_9', 'train_accs_10', 'train_accs_11', 'train_accs_12', 'train_accs_13', 'train_accs_14', 'train_accs_15', 'train_accs_16', 'train_accs_17', 'train_accs_18', 'train_accs_19', 'train_accs_20', 'train_accs_21', 'train_accs_22', 'train_accs_23', 'train_accs_24', 'train_accs_25', 'train_accs_26', 'train_accs_27', 'train_accs_28', 'train_accs_29', 'train_accs_30', 'train_accs_31', 'train_accs_32', 'train_accs_33', 'train_accs_34', 'train_accs_35', 'train_accs_36', 'train_accs_37', 'train_accs_38', 'train_accs_39', 'train_accs_40', 'train_accs_41', 'train_accs_42', 'train_accs_43', 'train_accs_44', 'train_accs_45', 'train_accs_46', 'train_accs_47', 'train_accs_48', 'train_accs_49', 'train_losses_0', 'train_losses_1', 'train_losses_2', 'train_losses_3', 'train_losses_4', 'train_losses_5', 'train_losses_6', 'train_losses_7', 'train_losses_8', 'train_losses_9', 'train_losses_10', 'train_losses_11', 'train_losses_12', 'train_losses_13', 'train_losses_14', 'train_losses_15', 'train_losses_16', 'train_losses_17', 'train_losses_18', 'train_losses_19', 'train_losses_20', 'train_losses_21', 'train_losses_22', 'train_losses_23', 'train_losses_24', 'train_losses_25', 'train_losses_26', 'train_losses_27', 'train_losses_28', 'train_losses_29', 'train_losses_30', 'train_losses_31', 'train_losses_32', 'train_losses_33', 'train_losses_34', 'train_losses_35', 'train_losses_36', 'train_losses_37', 'train_losses_38', 'train_losses_39', 'train_losses_40', 'train_losses_41', 'train_losses_42', 'train_losses_43', 'train_losses_44', 'train_losses_45', 'train_losses_46', 'train_losses_47', 'train_losses_48', 'train_losses_49']\n",
      "['val_accs_0', 'val_accs_1', 'val_accs_2', 'val_accs_3', 'val_accs_4', 'val_accs_5', 'val_accs_6', 'val_accs_7', 'val_accs_8', 'val_accs_9', 'val_accs_10', 'val_accs_11', 'val_accs_12', 'val_accs_13', 'val_accs_14', 'val_accs_15', 'val_accs_16', 'val_accs_17', 'val_accs_18', 'val_accs_19', 'val_accs_20', 'val_accs_21', 'val_accs_22', 'val_accs_23', 'val_accs_24', 'val_accs_25', 'val_accs_26', 'val_accs_27', 'val_accs_28', 'val_accs_29', 'val_accs_30', 'val_accs_31', 'val_accs_32', 'val_accs_33', 'val_accs_34', 'val_accs_35', 'val_accs_36', 'val_accs_37', 'val_accs_38', 'val_accs_39', 'val_accs_40', 'val_accs_41', 'val_accs_42', 'val_accs_43', 'val_accs_44', 'val_accs_45', 'val_accs_46', 'val_accs_47', 'val_accs_48', 'val_accs_49', 'val_losses_0', 'val_losses_1', 'val_losses_2', 'val_losses_3', 'val_losses_4', 'val_losses_5', 'val_losses_6', 'val_losses_7', 'val_losses_8', 'val_losses_9', 'val_losses_10', 'val_losses_11', 'val_losses_12', 'val_losses_13', 'val_losses_14', 'val_losses_15', 'val_losses_16', 'val_losses_17', 'val_losses_18', 'val_losses_19', 'val_losses_20', 'val_losses_21', 'val_losses_22', 'val_losses_23', 'val_losses_24', 'val_losses_25', 'val_losses_26', 'val_losses_27', 'val_losses_28', 'val_losses_29', 'val_losses_30', 'val_losses_31', 'val_losses_32', 'val_losses_33', 'val_losses_34', 'val_losses_35', 'val_losses_36', 'val_losses_37', 'val_losses_38', 'val_losses_39', 'val_losses_40', 'val_losses_41', 'val_losses_42', 'val_losses_43', 'val_losses_44', 'val_losses_45', 'val_losses_46', 'val_losses_47', 'val_losses_48', 'val_losses_49']\n",
      "              id  epochs  number_parameters  val_accs_0  val_accs_1  \\\n",
      "0        train_0     590             833577    0.292188    0.348047   \n",
      "1        train_1     556              55687    0.271875    0.529687   \n",
      "2        train_2     556             926455    0.282422    0.534375   \n",
      "3        train_3     550             873224    0.298438    0.498047   \n",
      "4        train_4     404             106291    0.281250    0.409766   \n",
      "5        train_5     556              71342    0.274219    0.533203   \n",
      "6        train_6     438             108388    0.285547    0.437109   \n",
      "7        train_7     404             723618    0.275000    0.467969   \n",
      "8        train_8     550             816928    0.271484    0.503125   \n",
      "9        train_9     438             782593    0.280078    0.541016   \n",
      "10      train_10     404              99460    0.292188    0.420703   \n",
      "11      train_11     632              80188    0.291406    0.427734   \n",
      "12      train_12     404              94578    0.289844    0.514844   \n",
      "13      train_13     547              50875    0.266406    0.587500   \n",
      "14      train_14     550             502947    0.281250    0.448437   \n",
      "15      train_15     422             895717    0.277344    0.366406   \n",
      "16      train_16     438              90408    0.263672    0.520312   \n",
      "17      train_17     589             952784    0.282813    0.268750   \n",
      "18      train_18     547             313589    0.285547    0.460547   \n",
      "19      train_19     547             612846    0.270313    0.541406   \n",
      "20      train_20     715              92500    0.250000    0.511328   \n",
      "21      train_21     422             415591    0.275781    0.418359   \n",
      "22      train_22     547             859383    0.273438    0.462500   \n",
      "23      train_23     547            1058771    0.270703    0.422266   \n",
      "24      train_24     547             722785    0.271875    0.483984   \n",
      "25      train_25     550              90101    0.273438    0.283203   \n",
      "26      train_26     918              97214    0.278125    0.275781   \n",
      "27      train_27     550             513998    0.275391    0.534375   \n",
      "28      train_28     404             710278    0.270703    0.291016   \n",
      "29      train_29     404              94093    0.277734    0.434766   \n",
      "...          ...     ...                ...         ...         ...   \n",
      "1848  train_1848     556             745615    0.277344    0.482812   \n",
      "1849  train_1849     404             713535    0.280078    0.492578   \n",
      "1850  train_1850     403              82393    0.272266    0.427734   \n",
      "1851  train_1851     550             130727    0.284375    0.432812   \n",
      "1852  train_1852     404              76250    0.287891    0.380078   \n",
      "1853  train_1853     770              92578    0.286719    0.462891   \n",
      "1854  train_1854     550             533394    0.285938    0.429297   \n",
      "1855  train_1855     542             190087    0.274609    0.282031   \n",
      "1856  train_1856     653              86334    0.272656    0.446094   \n",
      "1857  train_1857     556            1031918    0.267578    0.472266   \n",
      "1858  train_1858     550             737951    0.284375    0.425781   \n",
      "1859  train_1859     550             113401    0.291016    0.524219   \n",
      "1860  train_1860     993              92560    0.283594    0.451953   \n",
      "1861  train_1861     404             472616    0.284766    0.479297   \n",
      "1862  train_1862     550             992850    0.276563    0.461328   \n",
      "1863  train_1863     550              92693    0.274219    0.530469   \n",
      "1864  train_1864     550             775576    0.289844    0.492578   \n",
      "1865  train_1865     547              41134    0.282031    0.435156   \n",
      "1866  train_1866     550             777703    0.272656    0.469141   \n",
      "1867  train_1867     403             772566    0.275391    0.386719   \n",
      "1868  train_1868    1092              80240    0.294922    0.462500   \n",
      "1869  train_1869     556             476309    0.287891    0.424219   \n",
      "1870  train_1870     425             745950    0.282813    0.282422   \n",
      "1871  train_1871     420             746965    0.287500    0.358203   \n",
      "1872  train_1872     550             886119    0.279687    0.527344   \n",
      "1873  train_1873     547             977212    0.278906    0.311328   \n",
      "1874  train_1874     504             891389    0.273047    0.500000   \n",
      "1875  train_1875     547             106746    0.283594    0.451953   \n",
      "1876  train_1876     604             804473    0.310547    0.386719   \n",
      "1877  train_1877     403             102459    0.284766    0.342578   \n",
      "\n",
      "      val_accs_2  val_accs_3  val_accs_4  val_accs_5  val_accs_6  \\\n",
      "0       0.376172    0.374219    0.381250    0.357422    0.349609   \n",
      "1       0.553906    0.573438    0.576172    0.589063    0.582812   \n",
      "2       0.554297    0.584766    0.574609    0.583594    0.585156   \n",
      "3       0.533203    0.559375    0.560156    0.578125    0.578516   \n",
      "4       0.404297    0.408594    0.420703    0.420703    0.440625   \n",
      "5       0.569922    0.589844    0.613281    0.617188    0.648438   \n",
      "6       0.473438    0.472266    0.496094    0.493750    0.482812   \n",
      "7       0.491406    0.504297    0.510156    0.518750    0.505078   \n",
      "8       0.522656    0.527344    0.543359    0.541406    0.539062   \n",
      "9       0.570703    0.557422    0.585547    0.599609    0.588672   \n",
      "10      0.451953    0.456641    0.469141    0.477344    0.464453   \n",
      "11      0.439844    0.421484    0.436328    0.437500    0.455078   \n",
      "12      0.539062    0.537891    0.546094    0.557422    0.567578   \n",
      "13      0.617969    0.617969    0.631250    0.661328    0.658984   \n",
      "14      0.486719    0.505469    0.533594    0.523438    0.568750   \n",
      "15      0.333984    0.343750    0.363672    0.350781    0.348828   \n",
      "16      0.537109    0.552344    0.546484    0.546875    0.564844   \n",
      "17      0.283984    0.280078    0.282813    0.288672    0.279687   \n",
      "18      0.491797    0.504687    0.495312    0.500781    0.520312   \n",
      "19      0.568359    0.586719    0.597656    0.614062    0.631641   \n",
      "20      0.525781    0.535156    0.542188    0.553516    0.538672   \n",
      "21      0.435156    0.437500    0.454688    0.449609    0.438281   \n",
      "22      0.487891    0.504297    0.521875    0.520703    0.519922   \n",
      "23      0.447656    0.469141    0.485938    0.490234    0.497656   \n",
      "24      0.501563    0.495312    0.525781    0.520703    0.525781   \n",
      "25      0.282422    0.286328    0.278906    0.275781    0.279297   \n",
      "26      0.276563    0.270313    0.317969    0.275391    0.286328   \n",
      "27      0.559766    0.596094    0.597266    0.609766    0.613672   \n",
      "28      0.336719    0.341016    0.367188    0.393359    0.439844   \n",
      "29      0.456250    0.480469    0.494922    0.493750    0.520312   \n",
      "...          ...         ...         ...         ...         ...   \n",
      "1848    0.538281    0.553125    0.563672    0.573438    0.587500   \n",
      "1849    0.546875    0.556641    0.562500    0.564844    0.571875   \n",
      "1850    0.482812    0.489063    0.505078    0.496094    0.515234   \n",
      "1851    0.439063    0.443359    0.457031    0.450781    0.452734   \n",
      "1852    0.389062    0.393750    0.401562    0.430469    0.445703   \n",
      "1853    0.461328    0.483203    0.462891    0.486328    0.494141   \n",
      "1854    0.478125    0.492578    0.522266    0.525781    0.534766   \n",
      "1855    0.269531    0.278125    0.282813    0.283594    0.275000   \n",
      "1856    0.442969    0.447656    0.453516    0.458203    0.468750   \n",
      "1857    0.514062    0.550391    0.542578    0.555078    0.568750   \n",
      "1858    0.465625    0.459766    0.490625    0.491406    0.509766   \n",
      "1859    0.541406    0.575781    0.561719    0.574219    0.570312   \n",
      "1860    0.482422    0.496094    0.504687    0.497656    0.505078   \n",
      "1861    0.533984    0.532031    0.562891    0.581250    0.574219   \n",
      "1862    0.516016    0.527344    0.536719    0.559766    0.555859   \n",
      "1863    0.565625    0.564453    0.594141    0.596875    0.606641   \n",
      "1864    0.550000    0.580078    0.590625    0.603125    0.615234   \n",
      "1865    0.458203    0.483203    0.517969    0.522656    0.515234   \n",
      "1866    0.507422    0.509766    0.532031    0.558203    0.555469   \n",
      "1867    0.387500    0.421484    0.406250    0.387500    0.398438   \n",
      "1868    0.480859    0.487109    0.516016    0.512500    0.512500   \n",
      "1869    0.454297    0.478906    0.507031    0.524609    0.524609   \n",
      "1870    0.281250    0.283594    0.282031    0.316406    0.279687   \n",
      "1871    0.321484    0.361328    0.370703    0.368750    0.373828   \n",
      "1872    0.544922    0.567578    0.595313    0.582422    0.598828   \n",
      "1873    0.334766    0.353125    0.351562    0.347656    0.342969   \n",
      "1874    0.569141    0.569141    0.576562    0.589844    0.578516   \n",
      "1875    0.482812    0.517188    0.548828    0.543750    0.556250   \n",
      "1876    0.435156    0.421484    0.492578    0.495703    0.494922   \n",
      "1877    0.353516    0.378516    0.395703    0.386328    0.403516   \n",
      "\n",
      "           ...         train_losses_40  train_losses_41  train_losses_42  \\\n",
      "0          ...                2.273316         2.273133         2.273273   \n",
      "1          ...                1.923573         1.920186         1.916200   \n",
      "2          ...                0.947629         0.952411         0.935966   \n",
      "3          ...                1.773677         1.770966         1.768318   \n",
      "4          ...                2.063500         2.063436         2.059886   \n",
      "5          ...                1.047288         1.042632         1.037511   \n",
      "6          ...                1.813398         1.812301         1.817919   \n",
      "7          ...                1.530111         1.527063         1.526115   \n",
      "8          ...                1.671837         1.673483         1.666935   \n",
      "9          ...                0.959231         0.951043         0.944282   \n",
      "10         ...                1.790488         1.786511         1.783780   \n",
      "11         ...                1.665316         1.658952         1.650775   \n",
      "12         ...                1.844993         1.846939         1.844318   \n",
      "13         ...                0.628723         0.616147         0.603829   \n",
      "14         ...                1.978286         1.977573         1.979341   \n",
      "15         ...                2.281123         2.280284         2.278282   \n",
      "16         ...                1.484422         1.483243         1.486775   \n",
      "17         ...                2.302584         2.302585         2.302587   \n",
      "18         ...                1.741800         1.741158         1.735651   \n",
      "19         ...                0.371143         0.365333         0.355867   \n",
      "20         ...                1.177275         1.170246         1.164778   \n",
      "21         ...                2.248921         2.244647         2.246051   \n",
      "22         ...                1.508928         1.501806         1.500674   \n",
      "23         ...                1.722962         1.717986         1.713762   \n",
      "24         ...                1.939046         1.940335         1.928677   \n",
      "25         ...                2.302586         2.302592         2.302587   \n",
      "26         ...                2.287774         2.287288         2.287447   \n",
      "27         ...                0.987029         0.973444         0.970150   \n",
      "28         ...                2.229387         2.227495         2.227545   \n",
      "29         ...                1.762526         1.748859         1.748421   \n",
      "...        ...                     ...              ...              ...   \n",
      "1848       ...                0.746139         0.730726         0.708327   \n",
      "1849       ...                1.475891         1.475209         1.471645   \n",
      "1850       ...                1.546182         1.544179         1.543068   \n",
      "1851       ...                2.002951         1.996441         2.001445   \n",
      "1852       ...                2.270908         2.270849         2.270845   \n",
      "1853       ...                2.039106         2.035875         2.036474   \n",
      "1854       ...                1.601486         1.599114         1.594166   \n",
      "1855       ...                2.302644         2.302649         2.302639   \n",
      "1856       ...                1.805028         1.805575         1.802805   \n",
      "1857       ...                1.155968         1.148535         1.147353   \n",
      "1858       ...                1.985390         1.982356         1.983406   \n",
      "1859       ...                1.301212         1.299579         1.300977   \n",
      "1860       ...                1.942906         1.945864         1.942312   \n",
      "1861       ...                1.672634         1.655718         1.656786   \n",
      "1862       ...                1.277156         1.270260         1.265472   \n",
      "1863       ...                1.187036         1.184059         1.178952   \n",
      "1864       ...                0.082586         0.096119         0.085068   \n",
      "1865       ...                1.810449         1.806304         1.805891   \n",
      "1866       ...                1.861390         1.859646         1.858606   \n",
      "1867       ...                2.273320         2.271230         2.274720   \n",
      "1868       ...                1.961055         1.959611         1.955618   \n",
      "1869       ...                1.728017         1.718230         1.716760   \n",
      "1870       ...                2.301172         2.301105         2.301065   \n",
      "1871       ...                2.264219         2.267170         2.266889   \n",
      "1872       ...                0.852816         0.849656         0.835991   \n",
      "1873       ...                2.152688         2.148418         2.146360   \n",
      "1874       ...                1.368240         1.352972         1.345273   \n",
      "1875       ...                1.433645         1.425469         1.425259   \n",
      "1876       ...                1.274945         1.271599         1.264117   \n",
      "1877       ...                2.122256         2.122341         2.116721   \n",
      "\n",
      "      train_losses_43  train_losses_44  train_losses_45  train_losses_46  \\\n",
      "0            2.273123         2.273181         2.273089         2.273409   \n",
      "1            1.917743         1.911381         1.919590         1.919531   \n",
      "2            0.925001         0.918707         0.914364         0.898191   \n",
      "3            1.765915         1.764277         1.759141         1.758098   \n",
      "4            2.061260         2.059897         2.061916         2.060802   \n",
      "5            1.035069         1.033270         1.025444         1.027413   \n",
      "6            1.802642         1.806380         1.809569         1.802871   \n",
      "7            1.517122         1.514912         1.513046         1.514021   \n",
      "8            1.660911         1.662453         1.665102         1.663761   \n",
      "9            0.931646         0.926438         0.920068         0.915979   \n",
      "10           1.784227         1.777184         1.783406         1.778073   \n",
      "11           1.650616         1.647957         1.644248         1.638850   \n",
      "12           1.844218         1.839351         1.837790         1.834784   \n",
      "13           0.599396         0.592696         0.581573         0.570805   \n",
      "14           1.976354         1.974913         1.974607         1.974827   \n",
      "15           2.283330         2.281278         2.282656         2.281241   \n",
      "16           1.485840         1.485866         1.478404         1.475105   \n",
      "17           2.302585         2.302587         2.302585         2.302585   \n",
      "18           1.736925         1.735379         1.734878         1.726102   \n",
      "19           0.346097         0.335114         0.321632         0.324467   \n",
      "20           1.158045         1.157397         1.152117         1.147749   \n",
      "21           2.237333         2.243395         2.243253         2.243175   \n",
      "22           1.500851         1.501901         1.511145         1.507114   \n",
      "23           1.717130         1.706559         1.710832         1.706460   \n",
      "24           1.933550         1.932294         1.925771         1.926990   \n",
      "25           2.302585         2.302586         2.302588         2.302585   \n",
      "26           2.286292         2.285615         2.285789         2.285443   \n",
      "27           0.964219         0.945385         0.950271         0.938161   \n",
      "28           2.225819         2.227233         2.229159         2.221102   \n",
      "29           1.747637         1.747753         1.741657         1.740432   \n",
      "...               ...              ...              ...              ...   \n",
      "1848         0.687058         0.695177         0.677595         0.682145   \n",
      "1849         1.469088         1.465472         1.462569         1.457115   \n",
      "1850         1.537847         1.530446         1.538310         1.537432   \n",
      "1851         2.001645         1.997021         1.997867         1.998259   \n",
      "1852         2.270860         2.270763         2.270664         2.270882   \n",
      "1853         2.031285         2.037631         2.031155         2.033945   \n",
      "1854         1.589334         1.589998         1.586787         1.581319   \n",
      "1855         2.302624         2.302699         2.302619         2.302616   \n",
      "1856         1.808734         1.797822         1.801810         1.796982   \n",
      "1857         1.136074         1.128583         1.117653         1.105601   \n",
      "1858         1.982031         1.982481         1.977381         1.973325   \n",
      "1859         1.293816         1.293526         1.296372         1.291673   \n",
      "1860         1.943725         1.939656         1.936226         1.944158   \n",
      "1861         1.652190         1.646815         1.639691         1.632631   \n",
      "1862         1.253102         1.244132         1.240510         1.229003   \n",
      "1863         1.175739         1.175126         1.170056         1.166070   \n",
      "1864         0.066402         0.056676         0.045827         0.039459   \n",
      "1865         1.806748         1.810150         1.804214         1.801588   \n",
      "1866         1.857317         1.855631         1.852298         1.850519   \n",
      "1867         2.271514         2.272583         2.272704         2.272046   \n",
      "1868         1.954197         1.953983         1.949986         1.951911   \n",
      "1869         1.719730         1.717628         1.716915         1.713490   \n",
      "1870         2.300983         2.300940         2.300854         2.300797   \n",
      "1871         2.265016         2.264833         2.264869         2.264770   \n",
      "1872         0.821491         0.815069         0.808230         0.798462   \n",
      "1873         2.147887         2.148844         2.146595         2.146367   \n",
      "1874         1.344090         1.326210         1.331296         1.310993   \n",
      "1875         1.421844         1.421157         1.419637         1.415964   \n",
      "1876         1.255945         1.251034         1.246117         1.242710   \n",
      "1877         2.119756         2.113414         2.114638         2.114677   \n",
      "\n",
      "      train_losses_47  train_losses_48  train_losses_49  \n",
      "0            2.272900         2.273333         2.273214  \n",
      "1            1.917131         1.908027         1.913393  \n",
      "2            0.903883         0.891139         0.883376  \n",
      "3            1.755463         1.752970         1.750570  \n",
      "4            2.061664         2.055201         2.056667  \n",
      "5            1.028219         1.026870         1.026002  \n",
      "6            1.803938         1.804065         1.796791  \n",
      "7            1.512644         1.514058         1.511758  \n",
      "8            1.657770         1.660478         1.661498  \n",
      "9            0.910789         0.897131         0.889284  \n",
      "10           1.776503         1.781577         1.781234  \n",
      "11           1.640767         1.634756         1.626759  \n",
      "12           1.836535         1.837625         1.832365  \n",
      "13           0.574676         0.560220         0.551232  \n",
      "14           1.972036         1.970944         1.970362  \n",
      "15           2.283070         2.280723         2.282955  \n",
      "16           1.472824         1.469176         1.470574  \n",
      "17           2.302585         2.302586         2.302585  \n",
      "18           1.730372         1.733328         1.725827  \n",
      "19           0.317910         0.306828         0.301053  \n",
      "20           1.140679         1.136817         1.132978  \n",
      "21           2.245068         2.250006         2.243872  \n",
      "22           1.498502         1.496052         1.496141  \n",
      "23           1.700441         1.695275         1.701383  \n",
      "24           1.927864         1.923484         1.924525  \n",
      "25           2.302589         2.302587         2.302592  \n",
      "26           2.284653         2.284673         2.283463  \n",
      "27           0.927837         0.925352         0.923050  \n",
      "28           2.224936         2.225411         2.227736  \n",
      "29           1.743752         1.738560         1.733315  \n",
      "...               ...              ...              ...  \n",
      "1848         0.674866         0.655608         0.647107  \n",
      "1849         1.452685         1.457777         1.459155  \n",
      "1850         1.534013         1.525487         1.527817  \n",
      "1851         1.997813         1.997685         1.999489  \n",
      "1852         2.270473         2.270386         2.270400  \n",
      "1853         2.034024         2.026090         2.029157  \n",
      "1854         1.580331         1.579969         1.578227  \n",
      "1855         2.302630         2.302636         2.302810  \n",
      "1856         1.800128         1.797413         1.789166  \n",
      "1857         1.100346         1.099539         1.091049  \n",
      "1858         1.970769         1.969436         1.969682  \n",
      "1859         1.288229         1.288145         1.279066  \n",
      "1860         1.935663         1.932471         1.934249  \n",
      "1861         1.640593         1.630908         1.618799  \n",
      "1862         1.234234         1.217991         1.220699  \n",
      "1863         1.164154         1.159976         1.160179  \n",
      "1864         0.039246         0.049342         0.095971  \n",
      "1865         1.803579         1.802649         1.799676  \n",
      "1866         1.851690         1.848727         1.847180  \n",
      "1867         2.269361         2.272851         2.271293  \n",
      "1868         1.946835         1.946777         1.942897  \n",
      "1869         1.714255         1.713698         1.712141  \n",
      "1870         2.300735         2.300642         2.300569  \n",
      "1871         2.263264         2.263252         2.262895  \n",
      "1872         0.792093         0.786984         0.770494  \n",
      "1873         2.141046         2.139806         2.139841  \n",
      "1874         1.296605         1.294416         1.283409  \n",
      "1875         1.411966         1.406813         1.403312  \n",
      "1876         1.234381         1.227466         1.221015  \n",
      "1877         2.113445         2.117864         2.120155  \n",
      "\n",
      "[1878 rows x 203 columns]\n"
     ]
    }
   ],
   "source": [
    "col_train = [col for col in x.filter(regex='^train',axis=1).columns]\n",
    "col_val = [col for col in x.filter(regex='^val',axis=1).columns]\n",
    "print(col_train)\n",
    "print(col_val)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2887, 0.1016, 0.1139,  ..., 2.2729, 2.2733, 2.2732],\n",
      "        [0.2852, 0.2119, 0.2439,  ..., 1.9171, 1.9080, 1.9134],\n",
      "        [0.2828, 0.3212, 0.4224,  ..., 0.9039, 0.8911, 0.8834],\n",
      "        ...,\n",
      "        [0.2859, 0.2474, 0.3211,  ..., 1.4120, 1.4068, 1.4033],\n",
      "        [0.2969, 0.2540, 0.3042,  ..., 1.2344, 1.2275, 1.2210],\n",
      "        [0.2805, 0.1418, 0.1555,  ..., 2.1134, 2.1179, 2.1202]])\n",
      "tensor([[0.2922, 0.3480, 0.3762,  ..., 1.8060, 1.8070, 1.8077],\n",
      "        [0.2719, 0.5297, 0.5539,  ..., 1.2389, 1.2440, 1.2360],\n",
      "        [0.2824, 0.5344, 0.5543,  ..., 1.2594, 1.2777, 1.2688],\n",
      "        ...,\n",
      "        [0.2836, 0.4520, 0.4828,  ..., 1.1846, 1.1741, 1.1554],\n",
      "        [0.3105, 0.3867, 0.4352,  ..., 1.3494, 1.3871, 1.3924],\n",
      "        [0.2848, 0.3426, 0.3535,  ..., 1.5992, 1.5879, 1.5732]])\n",
      "torch.Size([1878, 100])\n",
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=25, out_features=1, bias=True)\n",
      "  (6): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch_tensor_train = torch.tensor([x[title].values for title in col_train],dtype=torch.float)\n",
    "torch_tensor_val = torch.tensor([x[title].values for title in col_val],dtype=torch.float)\n",
    "\n",
    "\n",
    "torch_tensor_train = torch.t(torch_tensor_train)\n",
    "torch_tensor_val = torch.t(torch_tensor_val)\n",
    "\n",
    "print(torch_tensor_train)\n",
    "print(torch_tensor_val)\n",
    "\n",
    "\n",
    "\n",
    "print(torch_tensor_train.size())\n",
    "\n",
    "output = model.train(torch_tensor_train[0])\n",
    "\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     train_accs_0  train_accs_1  train_accs_2  train_accs_3  train_accs_4  \\\n",
      "0        0.277734      0.101438      0.101019      0.102412      0.099996   \n",
      "1        0.288281      0.255748      0.326994      0.355921      0.374174   \n",
      "2        0.271484      0.117756      0.165095      0.195133      0.207787   \n",
      "3        0.273438      0.100325      0.100062      0.100032      0.101152   \n",
      "4        0.273047      0.108108      0.105339      0.106302      0.106452   \n",
      "5        0.271484      0.297170      0.349962      0.359111      0.368066   \n",
      "6        0.287500      0.102653      0.100442      0.099266      0.101138   \n",
      "7        0.277344      0.224777      0.314566      0.347820      0.369185   \n",
      "8        0.273828      0.133309      0.146547      0.149019      0.147699   \n",
      "9        0.262109      0.368811      0.500501      0.549374      0.579547   \n",
      "10       0.271875      0.293927      0.357583      0.379987      0.394889   \n",
      "11       0.273828      0.265263      0.336923      0.361242      0.376800   \n",
      "12       0.276953      0.379102      0.473489      0.518501      0.561254   \n",
      "13       0.270703      0.231900      0.274830      0.289797      0.296752   \n",
      "14       0.269531      0.262158      0.347382      0.380626      0.402668   \n",
      "15       0.262500      0.270320      0.347627      0.378795      0.396713   \n",
      "16       0.270313      0.303505      0.393507      0.423329      0.436350   \n",
      "17       0.300000      0.295095      0.359550      0.381977      0.396808   \n",
      "18       0.266797      0.263157      0.356831      0.396084      0.419737   \n",
      "19       0.271484      0.358170      0.434288      0.460397      0.476576   \n",
      "20       0.301953      0.371251      0.475140      0.518078      0.547851   \n",
      "21       0.266797      0.230843      0.293289      0.316861      0.327868   \n",
      "22       0.275391      0.097341      0.104251      0.116346      0.144565   \n",
      "23       0.284766      0.423032      0.543506      0.597052      0.626623   \n",
      "24       0.272656      0.199888      0.237171      0.251168      0.255601   \n",
      "25       0.277734      0.252196      0.285338      0.303202      0.311388   \n",
      "26       0.279297      0.100567      0.100873      0.122748      0.135287   \n",
      "27       0.278125      0.418560      0.551879      0.604564      0.653628   \n",
      "28       0.269141      0.311091      0.394162      0.442938      0.469028   \n",
      "29       0.287891      0.261618      0.321703      0.353508      0.370783   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "446      0.277344      0.274565      0.388460      0.427638      0.450513   \n",
      "447      0.281250      0.192782      0.246876      0.263855      0.276120   \n",
      "448      0.269922      0.333433      0.394953      0.409686      0.419954   \n",
      "449      0.276563      0.191032      0.255909      0.299324      0.322192   \n",
      "450      0.273438      0.123261      0.133145      0.168317      0.232405   \n",
      "451      0.287891      0.100731      0.099990      0.100009      0.096814   \n",
      "452      0.276172      0.368374      0.439746      0.473223      0.491600   \n",
      "453      0.284375      0.228783      0.288631      0.313462      0.324397   \n",
      "454      0.281250      0.316138      0.370514      0.389711      0.398658   \n",
      "455      0.274609      0.098550      0.099199      0.100129      0.102329   \n",
      "456      0.284375      0.154404      0.168844      0.197838      0.208685   \n",
      "457      0.281641      0.226840      0.286515      0.310645      0.324372   \n",
      "458      0.269531      0.243652      0.350844      0.397622      0.418320   \n",
      "459      0.291406      0.382552      0.509992      0.559119      0.596907   \n",
      "460      0.276953      0.188536      0.223336      0.241739      0.246310   \n",
      "461      0.284766      0.227900      0.297185      0.320825      0.331558   \n",
      "462      0.308594      0.330570      0.393554      0.412320      0.419508   \n",
      "463      0.275000      0.232183      0.300991      0.332979      0.345651   \n",
      "464      0.285156      0.245178      0.303704      0.314030      0.320342   \n",
      "465      0.275391      0.258824      0.305580      0.322436      0.332473   \n",
      "466      0.277734      0.243758      0.268193      0.276089      0.285044   \n",
      "467      0.282031      0.290457      0.354503      0.372791      0.388946   \n",
      "468      0.279687      0.255652      0.336834      0.374201      0.396553   \n",
      "469      0.283203      0.183836      0.212712      0.222668      0.223183   \n",
      "470      0.274219      0.179698      0.244429      0.289852      0.323246   \n",
      "471      0.271094      0.351159      0.460962      0.511798      0.544490   \n",
      "472      0.279687      0.160646      0.180499      0.195492      0.205576   \n",
      "473      0.269141      0.336106      0.427419      0.472332      0.501082   \n",
      "474      0.286328      0.305229      0.357582      0.380310      0.399360   \n",
      "475      0.283203      0.220623      0.296693      0.324147      0.345013   \n",
      "\n",
      "     train_accs_5  train_accs_6  train_accs_7  train_accs_8  train_accs_9  \\\n",
      "0        0.102174      0.102618      0.105825      0.106055      0.108654   \n",
      "1        0.387224      0.400166      0.407605      0.414278      0.424348   \n",
      "2        0.229728      0.236276      0.246163      0.252439      0.260821   \n",
      "3        0.107320      0.114264      0.113836      0.119252      0.115523   \n",
      "4        0.105297      0.107928      0.107029      0.108289      0.108845   \n",
      "5        0.376118      0.381176      0.387110      0.387116      0.388855   \n",
      "6        0.098442      0.099926      0.099562      0.103061      0.100049   \n",
      "7        0.380779      0.397874      0.404782      0.407498      0.416799   \n",
      "8        0.148198      0.157748      0.155700      0.150121      0.154238   \n",
      "9        0.604623      0.617837      0.626237      0.639246      0.644315   \n",
      "10       0.402457      0.408638      0.412371      0.421743      0.423269   \n",
      "11       0.381725      0.393280      0.398487      0.399738      0.406982   \n",
      "12       0.598910      0.637578      0.661811      0.690221      0.714795   \n",
      "13       0.301054      0.302723      0.303496      0.307197      0.306778   \n",
      "14       0.421621      0.427619      0.434386      0.443613      0.446815   \n",
      "15       0.406713      0.419030      0.424201      0.427045      0.428920   \n",
      "16       0.446950      0.455104      0.458289      0.468665      0.470382   \n",
      "17       0.403745      0.409362      0.415330      0.417895      0.420895   \n",
      "18       0.439826      0.453100      0.465899      0.470191      0.478134   \n",
      "19       0.488654      0.497730      0.507099      0.513081      0.517906   \n",
      "20       0.568241      0.584845      0.599299      0.611596      0.627785   \n",
      "21       0.335373      0.343352      0.350534      0.356013      0.359155   \n",
      "22       0.155780      0.160971      0.163110      0.171391      0.166362   \n",
      "23       0.649849      0.664685      0.681900      0.696407      0.704951   \n",
      "24       0.258173      0.265424      0.267725      0.268590      0.274702   \n",
      "25       0.323803      0.327618      0.333827      0.339501      0.342055   \n",
      "26       0.139607      0.141196      0.143951      0.145405      0.147500   \n",
      "27       0.697833      0.743088      0.783144      0.823623      0.864739   \n",
      "28       0.490739      0.511400      0.524858      0.537020      0.550201   \n",
      "29       0.388853      0.394359      0.403548      0.410433      0.415110   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "446      0.467333      0.485169      0.496484      0.506428      0.518149   \n",
      "447      0.280192      0.286001      0.290440      0.293487      0.295201   \n",
      "448      0.426845      0.431464      0.432651      0.440597      0.441771   \n",
      "449      0.334776      0.347717      0.355328      0.362795      0.362555   \n",
      "450      0.273185      0.285246      0.297654      0.302789      0.308667   \n",
      "451      0.100346      0.099888      0.096319      0.098570      0.098722   \n",
      "452      0.512978      0.523350      0.539566      0.556165      0.564392   \n",
      "453      0.330119      0.340643      0.340230      0.344306      0.349296   \n",
      "454      0.412428      0.416973      0.423491      0.433886      0.435625   \n",
      "455      0.096863      0.098788      0.098261      0.100231      0.100640   \n",
      "456      0.212258      0.231354      0.262295      0.268302      0.275886   \n",
      "457      0.332770      0.338263      0.341043      0.349840      0.353966   \n",
      "458      0.439330      0.449904      0.466310      0.479206      0.484808   \n",
      "459      0.632154      0.662050      0.687615      0.713313      0.737544   \n",
      "460      0.243761      0.247591      0.249570      0.250630      0.250918   \n",
      "461      0.345276      0.352356      0.358721      0.365467      0.368892   \n",
      "462      0.430599      0.440331      0.446040      0.453833      0.463687   \n",
      "463      0.361856      0.366286      0.380904      0.384755      0.390963   \n",
      "464      0.321316      0.321729      0.325302      0.328623      0.327496   \n",
      "465      0.352495      0.362686      0.369127      0.372354      0.377513   \n",
      "466      0.285722      0.286975      0.291236      0.293304      0.295958   \n",
      "467      0.394786      0.402128      0.409668      0.415105      0.419010   \n",
      "468      0.412555      0.425192      0.435032      0.445034      0.453245   \n",
      "469      0.229867      0.231152      0.234361      0.238398      0.241663   \n",
      "470      0.342056      0.351890      0.359523      0.368264      0.376393   \n",
      "471      0.576442      0.600662      0.609439      0.625983      0.636863   \n",
      "472      0.208331      0.208727      0.212573      0.212311      0.216581   \n",
      "473      0.523387      0.536421      0.546581      0.558554      0.570025   \n",
      "474      0.411807      0.420282      0.425921      0.432474      0.436178   \n",
      "475      0.355461      0.364055      0.373721      0.378666      0.381270   \n",
      "\n",
      "          ...         train_losses_40  train_losses_41  train_losses_42  \\\n",
      "0         ...                2.242073         2.241485         2.241650   \n",
      "1         ...                1.963215         1.963528         1.963080   \n",
      "2         ...                1.948226         1.944007         1.936107   \n",
      "3         ...                2.294686         2.294415         2.293760   \n",
      "4         ...                2.219885         2.216731         2.220044   \n",
      "5         ...                1.731665         1.729826         1.729109   \n",
      "6         ...                2.302632         2.302780         2.302630   \n",
      "7         ...                1.469054         1.464146         1.458834   \n",
      "8         ...                2.148987         2.150121         2.150369   \n",
      "9         ...                0.830583         0.834251         0.824921   \n",
      "10        ...                1.486268         1.476975         1.488979   \n",
      "11        ...                1.484012         1.477606         1.476133   \n",
      "12        ...                0.021001         0.009928         0.006316   \n",
      "13        ...                1.921239         1.923544         1.923597   \n",
      "14        ...                1.321728         1.318307         1.310965   \n",
      "15        ...                1.591400         1.588274         1.590058   \n",
      "16        ...                1.269524         1.268989         1.259293   \n",
      "17        ...                1.993312         1.990430         1.988907   \n",
      "18        ...                1.239066         1.231364         1.226439   \n",
      "19        ...                1.017986         1.007529         0.998992   \n",
      "20        ...                0.446913         0.444396         0.431614   \n",
      "21        ...                2.043526         2.042841         2.044348   \n",
      "22        ...                2.220383         2.219876         2.218200   \n",
      "23        ...                0.375736         0.372394         0.363575   \n",
      "24        ...                1.948699         1.941379         1.938907   \n",
      "25        ...                1.792500         1.789769         1.790206   \n",
      "26        ...                2.272255         2.274168         2.275587   \n",
      "27        ...                0.080443         0.029242         0.015324   \n",
      "28        ...                0.706256         0.706789         0.699493   \n",
      "29        ...                1.413175         1.406048         1.400406   \n",
      "..        ...                     ...              ...              ...   \n",
      "446       ...                0.622961         0.599708         0.582650   \n",
      "447       ...                1.816012         1.811003         1.811688   \n",
      "448       ...                1.482925         1.486478         1.485904   \n",
      "449       ...                1.624029         1.612436         1.610005   \n",
      "450       ...                1.725173         1.720372         1.727797   \n",
      "451       ...                2.302593         2.302593         2.302580   \n",
      "452       ...                0.572534         0.551569         0.531423   \n",
      "453       ...                1.642298         1.640335         1.639552   \n",
      "454       ...                1.397706         1.393989         1.393135   \n",
      "455       ...                2.276208         2.274339         2.270901   \n",
      "456       ...                2.015542         2.012050         2.006339   \n",
      "457       ...                1.680135         1.681502         1.672241   \n",
      "458       ...                0.781725         0.768766         0.757055   \n",
      "459       ...                0.082916         0.077885         0.070293   \n",
      "460       ...                2.117004         2.114667         2.121270   \n",
      "461       ...                2.036523         2.038671         2.036430   \n",
      "462       ...                2.229819         2.229184         2.228694   \n",
      "463       ...                1.481117         1.483776         1.473933   \n",
      "464       ...                2.267945         2.268148         2.267678   \n",
      "465       ...                2.044122         2.042605         2.042471   \n",
      "466       ...                1.896720         1.895800         1.893948   \n",
      "467       ...                1.639375         1.636788         1.636134   \n",
      "468       ...                1.331567         1.331747         1.315186   \n",
      "469       ...                1.938618         1.936901         1.935856   \n",
      "470       ...                1.489395         1.484221         1.473640   \n",
      "471       ...                0.625128         0.622510         0.614266   \n",
      "472       ...                2.150012         2.145659         2.143731   \n",
      "473       ...                1.150945         1.147222         1.140918   \n",
      "474       ...                1.375306         1.374035         1.373287   \n",
      "475       ...                1.561030         1.565204         1.563676   \n",
      "\n",
      "     train_losses_43  train_losses_44  train_losses_45  train_losses_46  \\\n",
      "0           2.239981         2.237806         2.239618         2.238551   \n",
      "1           1.963272         1.958386         1.961321         1.958751   \n",
      "2           1.935211         1.936958         1.936536         1.927323   \n",
      "3           2.293803         2.294037         2.294025         2.293376   \n",
      "4           2.216306         2.217262         2.218061         2.215481   \n",
      "5           1.728663         1.724304         1.722235         1.722843   \n",
      "6           2.302695         2.302661         2.302746         2.302495   \n",
      "7           1.457065         1.457603         1.447911         1.443568   \n",
      "8           2.147058         2.151580         2.148709         2.148557   \n",
      "9           0.824155         0.824259         0.819359         0.817494   \n",
      "10          1.484686         1.477127         1.473635         1.478146   \n",
      "11          1.469955         1.470618         1.464928         1.466583   \n",
      "12          0.004717         0.003933         0.003333         0.002862   \n",
      "13          1.924712         1.926097         1.925583         1.926528   \n",
      "14          1.304384         1.301772         1.297451         1.286807   \n",
      "15          1.585551         1.587018         1.583342         1.583045   \n",
      "16          1.259077         1.250567         1.245428         1.245355   \n",
      "17          1.991004         1.990408         1.990035         1.990334   \n",
      "18          1.217658         1.213489         1.207128         1.202970   \n",
      "19          0.995277         0.991034         0.980127         0.976587   \n",
      "20          0.408915         0.403337         0.393085         0.385251   \n",
      "21          2.040267         2.040505         2.036886         2.039179   \n",
      "22          2.218570         2.216603         2.216770         2.215757   \n",
      "23          0.356378         0.346366         0.344695         0.327069   \n",
      "24          1.943249         1.938163         1.929929         1.935467   \n",
      "25          1.785878         1.788751         1.791783         1.790283   \n",
      "26          2.272264         2.272023         2.274212         2.277813   \n",
      "27          0.010517         0.008474         0.007069         0.006019   \n",
      "28          0.678668         0.666877         0.651889         0.650250   \n",
      "29          1.398023         1.385493         1.390091         1.385391   \n",
      "..               ...              ...              ...              ...   \n",
      "446         0.572741         0.557030         0.540529         0.530447   \n",
      "447         1.807135         1.807547         1.797266         1.801748   \n",
      "448         1.484386         1.478388         1.479848         1.481094   \n",
      "449         1.616513         1.607979         1.612940         1.608098   \n",
      "450         1.724610         1.713875         1.710986         1.711937   \n",
      "451         2.302593         2.302620         2.302619         2.302598   \n",
      "452         0.518586         0.502914         0.488407         0.472405   \n",
      "453         1.638499         1.634256         1.630351         1.626829   \n",
      "454         1.387912         1.385063         1.386163         1.379368   \n",
      "455         2.271824         2.270327         2.269226         2.264557   \n",
      "456         2.007621         2.004357         2.000608         1.999174   \n",
      "457         1.675688         1.670158         1.661289         1.665356   \n",
      "458         0.750118         0.722257         0.704798         0.687189   \n",
      "459         0.071137         0.072925         0.075874         0.067318   \n",
      "460         2.120496         2.119972         2.119802         2.117806   \n",
      "461         2.034532         2.035116         2.036358         2.034427   \n",
      "462         2.228988         2.228086         2.228115         2.227575   \n",
      "463         1.473500         1.466655         1.460282         1.457426   \n",
      "464         2.267366         2.267662         2.267511         2.267604   \n",
      "465         2.039462         2.040895         2.038887         2.038257   \n",
      "466         1.893724         1.893897         1.882617         1.891153   \n",
      "467         1.635935         1.629663         1.629467         1.630779   \n",
      "468         1.318486         1.318500         1.311649         1.311608   \n",
      "469         1.935344         1.934381         1.927748         1.929331   \n",
      "470         1.476122         1.463392         1.463683         1.455507   \n",
      "471         0.610755         0.597808         0.591081         0.591788   \n",
      "472         2.143450         2.139167         2.142349         2.135220   \n",
      "473         1.133930         1.130718         1.123360         1.119474   \n",
      "474         1.369986         1.358178         1.353384         1.347333   \n",
      "475         1.559141         1.555614         1.555997         1.553298   \n",
      "\n",
      "     train_losses_47  train_losses_48  train_losses_49  \n",
      "0           2.239813         2.236169         2.236570  \n",
      "1           1.956550         1.954626         1.951902  \n",
      "2           1.935898         1.930658         1.926211  \n",
      "3           2.294148         2.293525         2.292587  \n",
      "4           2.217582         2.217886         2.218051  \n",
      "5           1.724321         1.718823         1.726017  \n",
      "6           2.302666         2.302697         2.302717  \n",
      "7           1.444143         1.442554         1.439947  \n",
      "8           2.147402         2.148117         2.150682  \n",
      "9           0.815674         0.817155         0.813962  \n",
      "10          1.478274         1.474715         1.467979  \n",
      "11          1.460370         1.454688         1.445730  \n",
      "12          0.002665         0.002590         0.002363  \n",
      "13          1.924906         1.922356         1.926280  \n",
      "14          1.278468         1.273961         1.262502  \n",
      "15          1.583352         1.579610         1.580596  \n",
      "16          1.234283         1.232604         1.229747  \n",
      "17          1.987830         1.985664         1.986306  \n",
      "18          1.191652         1.187959         1.181537  \n",
      "19          0.963804         0.955916         0.956031  \n",
      "20          0.379187         0.370935         0.363941  \n",
      "21          2.036507         2.040734         2.035105  \n",
      "22          2.215437         2.212790         2.212790  \n",
      "23          0.331518         0.322277         0.314197  \n",
      "24          1.933882         1.937913         1.935099  \n",
      "25          1.786884         1.783852         1.784404  \n",
      "26          2.279805         2.272863         2.273214  \n",
      "27          0.005383         0.004926         0.004524  \n",
      "28          0.627806         0.631631         0.613814  \n",
      "29          1.385900         1.378099         1.382847  \n",
      "..               ...              ...              ...  \n",
      "446         0.528722         0.491904         0.489221  \n",
      "447         1.806179         1.796670         1.795507  \n",
      "448         1.478853         1.480638         1.472591  \n",
      "449         1.608409         1.607160         1.602301  \n",
      "450         1.709154         1.710472         1.704151  \n",
      "451         2.302596         2.302614         2.302614  \n",
      "452         0.459779         0.445321         0.430759  \n",
      "453         1.626469         1.621072         1.621097  \n",
      "454         1.377909         1.377563         1.377034  \n",
      "455         2.264197         2.261728         2.258824  \n",
      "456         1.995025         1.994043         1.990603  \n",
      "457         1.657448         1.657693         1.654571  \n",
      "458         0.683332         0.665236         0.648326  \n",
      "459         0.063611         0.060956         0.059684  \n",
      "460         2.119144         2.117268         2.115441  \n",
      "461         2.035324         2.030141         2.035190  \n",
      "462         2.227276         2.227263         2.226853  \n",
      "463         1.442103         1.444618         1.447291  \n",
      "464         2.267445         2.267095         2.267310  \n",
      "465         2.038065         2.035518         2.036229  \n",
      "466         1.892189         1.889642         1.893492  \n",
      "467         1.626135         1.628151         1.626302  \n",
      "468         1.306683         1.305086         1.296863  \n",
      "469         1.931176         1.927192         1.926334  \n",
      "470         1.449189         1.442393         1.435624  \n",
      "471         0.583976         0.574070         0.570866  \n",
      "472         2.135979         2.138529         2.143163  \n",
      "473         1.112423         1.108147         1.104092  \n",
      "474         1.349066         1.347365         1.345782  \n",
      "475         1.556681         1.544511         1.546347  \n",
      "\n",
      "[476 rows x 100 columns]\n",
      "tensor([[0.2777, 0.2883, 0.2715,  ..., 0.2691, 0.2863, 0.2832],\n",
      "        [0.1014, 0.2557, 0.1178,  ..., 0.3361, 0.3052, 0.2206],\n",
      "        [0.1010, 0.3270, 0.1651,  ..., 0.4274, 0.3576, 0.2967],\n",
      "        ...,\n",
      "        [2.2398, 1.9565, 1.9359,  ..., 1.1124, 1.3491, 1.5567],\n",
      "        [2.2362, 1.9546, 1.9307,  ..., 1.1081, 1.3474, 1.5445],\n",
      "        [2.2366, 1.9519, 1.9262,  ..., 1.1041, 1.3458, 1.5463]])\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"test.csv\", index_col = 0)\n",
    "test_data = test_data.drop(columns = ['batch_size_test', 'batch_size_val', 'criterion', 'optimizer', 'batch_size_train'])\n",
    "test_data = test_data.drop(columns = ['arch_and_hp','init_params_mu', 'init_params_std', 'init_params_l2'])\n",
    "test_train = test_data[col_train]\n",
    "test_val = test_data[col_val]\n",
    "\n",
    "torch_tensor_test_train = torch.tensor([test_train[title].values for title in col_train],dtype=torch.float)\n",
    "torch_tensor_test_val = torch.tensor([test_val[title].values for title in col_val],dtype=torch.float)\n",
    "print(test_train)\n",
    "print(torch_tensor_test_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xPredicted = torch.tensor(  ([4, 8])  , dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
